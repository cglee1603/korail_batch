"""
ë°°ì¹˜ í”„ë¡œì„¸ì„œ - ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¡°ìœ¨
"""
import time
from pathlib import Path
from typing import Dict, List, Optional
from excel_processor import ExcelProcessor, SheetType
from file_handler import FileHandler
from ragflow_client import RAGFlowClient  # HTTP API í´ë¼ì´ì–¸íŠ¸
from revision_db import RevisionDB  # Revision ê´€ë¦¬ DB
from logger import logger
from config import (
    EXCEL_FILE_PATH, 
    DATASET_PERMISSION, 
    EMBEDDING_MODEL,
    DATA_SOURCE,
    DB_CONNECTION_STRING,
    CHUNK_METHOD,
    PARSER_CONFIG,
    AUTO_PARSE_AFTER_UPLOAD,
    MONITOR_PARSE_PROGRESS,
    PARSE_TIMEOUT_MINUTES,
    ENABLE_REVISION_MANAGEMENT,
    SKIP_SAME_REVISION,
    DELETE_BEFORE_UPLOAD,
    PURGE_BEFORE_HISTORY_SOFTWARE,
    HISTORY_SHEET_UPLOAD_FORMAT,
    TEMP_DIR
)


class BatchProcessor:
    """ë°°ì¹˜ ì²˜ë¦¬ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, excel_path: str = None, data_source: str = None, filesystem_path: str = None):
        """
        Args:
            excel_path: ì—‘ì…€ íŒŒì¼ ê²½ë¡œ
            data_source: ë°ì´í„° ì†ŒìŠ¤ ("excel", "db", "filesystem", "both" ë“± ì½¤ë§ˆ êµ¬ë¶„)
            filesystem_path: íŒŒì¼ì‹œìŠ¤í…œ ë£¨íŠ¸ ê²½ë¡œ (filesystem ëª¨ë“œìš©)
        """
        self.excel_path = excel_path or EXCEL_FILE_PATH
        
        # ë°ì´í„° ì†ŒìŠ¤ íŒŒì‹± (ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ê°’ ì§€ì›)
        raw_source = data_source or DATA_SOURCE
        self.data_sources = [s.strip().lower() for s in raw_source.split(',')]
        
        self.data_source = raw_source  # ë¡œê¹…ìš© ì›ë³¸ ë¬¸ìì—´
        self.filesystem_path = filesystem_path

        # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.excel_processor = None
        self.db_processor = None
        self.filesystem_processor = None
        
        # Excel ì†ŒìŠ¤
        if 'excel' in self.data_sources:
            self.excel_processor = ExcelProcessor(self.excel_path)
        
        # Revision ê´€ë¦¬ DB ë¨¼ì € ì´ˆê¸°í™” (FileHandlerì—ì„œ ì‚¬ìš©)
        self.revision_db = RevisionDB()
        
        # ë‹¤ìš´ë¡œë“œ ìºì‹œ ìë™ ì •ë¦¬ (ì„¤ì •ëœ ê²½ìš°)
        from config import AUTO_CLEAN_DOWNLOAD_CACHE, DOWNLOAD_CACHE_KEEP_DAYS
        if AUTO_CLEAN_DOWNLOAD_CACHE:
            logger.info(f"ë‹¤ìš´ë¡œë“œ ìºì‹œ ìë™ ì •ë¦¬ ì‹œì‘ (ë³´ê´€ ê¸°ê°„: {DOWNLOAD_CACHE_KEEP_DAYS}ì¼)")
            if DOWNLOAD_CACHE_KEEP_DAYS > 0:
                deleted = self.revision_db.clear_mt_download_cache(older_than_days=DOWNLOAD_CACHE_KEEP_DAYS, delete_files=True)
                logger.info(f"âœ“ {DOWNLOAD_CACHE_KEEP_DAYS}ì¼ ì´ìƒëœ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {deleted}ê°œ")
            else:
                deleted = self.revision_db.clear_mt_download_cache(older_than_days=None, delete_files=True)
                logger.info(f"âœ“ ì „ì²´ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {deleted}ê°œ")
        
        # ì•”ë³µí˜¸í™” í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
        from crypto_handler import CryptoHandler
        self.crypto_handler = CryptoHandler()
        
        # FileHandler ì´ˆê¸°í™” (ë‹¤ìš´ë¡œë“œ ìºì‹œ + ì•”ë³µí˜¸í™”)
        self.file_handler = FileHandler(
            revision_db=self.revision_db,
            crypto_handler=self.crypto_handler
        )

        # FilesystemProcessor ì´ˆê¸°í™” (FileHandler ìƒì„± í›„)
        if 'filesystem' in self.data_sources and self.filesystem_path:
            from filesystem_processor import FilesystemProcessor
            self.filesystem_processor = FilesystemProcessor(
                root_path=self.filesystem_path,
                revision_db=self.revision_db,
                file_handler=self.file_handler
            )
        
        # DB ì†ŒìŠ¤
        if 'db' in self.data_sources:
            self._init_db_processor()
        
        self.ragflow_client = RAGFlowClient()
        
        self.stats = {
            'total_sheets': 0,
            'skipped_sheets': 0,  # ëª©ì°¨ ë“±
            'revision_sheets': 0,  # REV/ì‘ì„±ë²„ì „ ê´€ë¦¬
            'attachment_sheets': 0,  # ì²¨ë¶€íŒŒì¼
            'history_sheets': 0,  # ì´ë ¥ê´€ë¦¬+ì†Œí”„íŠ¸ì›¨ì–´
            
            'new_documents': 0,  # ì‹ ê·œ ë¬¸ì„œ
            'updated_documents': 0,  # ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œ
            'skipped_documents': 0,  # ë™ì¼ revision
            'deleted_documents': 0,  # ì‚­ì œëœ ë¬¸ì„œ
            'failed_deletions': 0,  # ì‚­ì œ ì‹¤íŒ¨
            
            'total_files': 0,
            'successful_uploads': 0,
            'failed_uploads': 0,
            'datasets_created': 0
        }
    
    def _init_db_processor(self):
        """DB í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”"""
        try:
            from db_connector import DBConnector
            from db_processor import DBProcessor
            
            # DB ì—°ê²° ë¬¸ìì—´ í™•ì¸
            if not DB_CONNECTION_STRING:
                logger.warning("DB ì—°ê²° ë¬¸ìì—´ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. DB ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                self.data_source = 'excel'  # ê°•ì œë¡œ Excelë§Œ ì²˜ë¦¬
                return
            
            connector = DBConnector(connection_string=DB_CONNECTION_STRING)
            # FileHandlerë¥¼ ì „ë‹¬í•˜ì—¬ DB ë°ì´í„°ë¥¼ PDFë¡œ ë³€í™˜
            self.db_processor = DBProcessor(connector, file_handler=self.file_handler)
            logger.info(f"DB í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ (PDF ë³€í™˜ ì§€ì›)")
        
        except ImportError as e:
            logger.error(f"DB ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            logger.error("í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install sqlalchemy psycopg2-binary pymysql")
            if 'db' in self.data_sources:
                self.data_sources.remove('db')
        except Exception as e:
            logger.error(f"DB í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if 'db' in self.data_sources:
                self.data_sources.remove('db')
    
    def is_revision_newer(self, old_rev: str, new_rev: str) -> bool:
        """
        ë‘ revisionì„ ë¹„êµí•˜ì—¬ ìƒˆ ë²„ì „ì¸ì§€ íŒë‹¨
        
        Args:
            old_rev: ê¸°ì¡´ revision
            new_rev: ìƒˆ revision
        
        Returns:
            True if new_revê°€ old_revë³´ë‹¤ ìµœì‹ 
            
        Note:
            - REV í˜•ì‹: A, A1, C1, D4 (ì•ŒíŒŒë²³ + ìˆ«ì)
            - ì‘ì„±ë²„ì „ í˜•ì‹: R1, R0, R16 (R + ìˆ«ì)
            - ì  ë²„ì „: 1.0, 2.0, 1.1.0
        """
        if old_rev == new_rev:
            return False
        
        import re
        
        try:
            # 1. ì‘ì„±ë²„ì „ í˜•ì‹: R + ìˆ«ì (ì˜ˆ: R1, R0, R16)
            if old_rev.upper().startswith('R') and new_rev.upper().startswith('R'):
                try:
                    old_num = int(old_rev[1:])
                    new_num = int(new_rev[1:])
                    result = new_num > old_num
                    logger.debug(f"ì‘ì„±ë²„ì „ ë¹„êµ: {old_rev}({old_num}) vs {new_rev}({new_num}) â†’ {'ìµœì‹ ' if result else 'ë™ì¼/ì´ì „'}")
                    return result
                except ValueError:
                    pass
            
            # 2. REV í˜•ì‹: ì•ŒíŒŒë²³ + ìˆ«ì (ì˜ˆ: A, A1, C1, D4)
            # íŒ¨í„´: ì•ŒíŒŒë²³(ëŒ€ë¬¸ì) + ì„ íƒì  ìˆ«ì
            rev_pattern = re.compile(r'^([A-Z]+)(\d*)$', re.IGNORECASE)
            old_match = rev_pattern.match(old_rev)
            new_match = rev_pattern.match(new_rev)
            
            if old_match and new_match:
                old_letter = old_match.group(1).upper()
                old_number = int(old_match.group(2)) if old_match.group(2) else 0
                new_letter = new_match.group(1).upper()
                new_number = int(new_match.group(2)) if new_match.group(2) else 0
                
                # ì•ŒíŒŒë²³ ë¨¼ì € ë¹„êµ
                if new_letter > old_letter:
                    logger.debug(f"REV ë¹„êµ: {old_rev}({old_letter}{old_number}) vs {new_rev}({new_letter}{new_number}) â†’ ìµœì‹  (ì•ŒíŒŒë²³)")
                    return True
                elif new_letter < old_letter:
                    logger.debug(f"REV ë¹„êµ: {old_rev}({old_letter}{old_number}) vs {new_rev}({new_letter}{new_number}) â†’ ì´ì „ (ì•ŒíŒŒë²³)")
                    return False
                else:
                    # ì•ŒíŒŒë²³ì´ ê°™ìœ¼ë©´ ìˆ«ì ë¹„êµ
                    result = new_number > old_number
                    logger.debug(f"REV ë¹„êµ: {old_rev}({old_letter}{old_number}) vs {new_rev}({new_letter}{new_number}) â†’ {'ìµœì‹ ' if result else 'ë™ì¼/ì´ì „'} (ìˆ«ì)")
                    return result
            
            # 3. ì  ë²„ì „ í˜•ì‹ ë¹„êµ (1.0, 2.0, 1.1.0)
            if '.' in old_rev or '.' in new_rev:
                old_parts = old_rev.split('.')
                new_parts = new_rev.split('.')
                
                # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ê²½ìš°
                try:
                    for i in range(max(len(old_parts), len(new_parts))):
                        old_num = int(old_parts[i]) if i < len(old_parts) else 0
                        new_num = int(new_parts[i]) if i < len(new_parts) else 0
                        
                        if new_num > old_num:
                            return True
                        elif new_num < old_num:
                            return False
                    
                    # ëª¨ë‘ ê°™ìœ¼ë©´ False
                    return False
                
                except (ValueError, IndexError):
                    # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨
                    pass
            
            # 4. ìˆœìˆ˜ ìˆ«ì ë¹„êµ
            try:
                return float(new_rev) > float(old_rev)
            except ValueError:
                pass
            
            # 5. ë¬¸ìì—´ ì‚¬ì „ì‹ ë¹„êµ (í´ë°±)
            logger.debug(f"Revision ë¹„êµ (ì‚¬ì „ì‹): {old_rev} vs {new_rev}")
            return new_rev > old_rev
        
        except Exception as e:
            logger.warning(f"Revision ë¹„êµ ì‹¤íŒ¨ (old: {old_rev}, new: {new_rev}): {e}")
            # ë¹„êµ ì‹¤íŒ¨ ì‹œ ì—…ë°ì´íŠ¸ë¡œ ê°„ì£¼
            return True
    
    def process(self):
        """ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("="*80)
        logger.info("ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        logger.info(f"ë°ì´í„° ì†ŒìŠ¤: {self.data_source.upper()}")
        if 'excel' in self.data_sources:
            logger.info(f"ì—‘ì…€ íŒŒì¼: {self.excel_path}")
        if 'filesystem' in self.data_sources and self.filesystem_path:
            logger.info(f"íŒŒì¼ì‹œìŠ¤í…œ ê²½ë¡œ: {self.filesystem_path}")
        logger.info(f"Revision ê´€ë¦¬: {'í™œì„±í™”' if ENABLE_REVISION_MANAGEMENT else 'ë¹„í™œì„±í™”'}")
        logger.info("="*80)
        
        try:
            # ë°ì´í„° ìˆ˜ì§‘
            all_data = {}
            
            # 1. Excel ë°ì´í„° ì¶”ì¶œ
            if 'excel' in self.data_sources and self.excel_processor:
                logger.info("\n[Excel ë°ì´í„° ì²˜ë¦¬]")
                sheet_data = self.excel_processor.process_all_sheets()
                all_data.update(sheet_data)
                self.stats['total_sheets'] += len(sheet_data)
            
            # 2. DB ë°ì´í„° ì¶”ì¶œ
            if 'db' in self.data_sources and self.db_processor:
                logger.info("\n[DB ë°ì´í„° ì²˜ë¦¬]")
                db_data = self.db_processor.process(query_name="DB_Query")
                # DB ë°ì´í„°ëŠ” ê¸°ì¡´ í˜•ì‹ì´ë¯€ë¡œ ë³€í™˜
                for sheet_name, items in db_data.items():
                    all_data[sheet_name] = (SheetType.ATTACHMENT, items, [])
                self.stats['total_sheets'] += len(db_data)

            # 3. Filesystem ì²˜ë¦¬ (ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰)
            if 'filesystem' in self.data_sources and self.filesystem_processor:
                logger.info("\n[Filesystem ë°ì´í„° ì²˜ë¦¬]")
                self.filesystem_processor.process()
                # í†µê³„ ë³‘í•©
                fs_stats = self.filesystem_processor.stats
                self.stats['datasets_created'] += fs_stats['datasets_created']
                self.stats['total_files'] += fs_stats['total_files']
                self.stats['new_documents'] += fs_stats['new_files']
                self.stats['updated_documents'] += fs_stats['updated_files']
                self.stats['skipped_documents'] += fs_stats['skipped_files']
                self.stats['failed_uploads'] += fs_stats['failed_files']
            
            if not all_data and 'filesystem' not in self.data_sources:
                logger.error("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 4. ì‹œíŠ¸ íƒ€ì…ë³„ë¡œ ì²˜ë¦¬ (Excel/DB ë°ì´í„°)
            for sheet_name, (sheet_type, items, headers) in all_data.items():
                logger.info(f"\n{'='*60}")
                logger.info(f"ì‹œíŠ¸ ì²˜ë¦¬ ì‹œì‘: {sheet_name} (íƒ€ì…: {sheet_type.value})")
                logger.info(f"{'='*60}")
                
                # ì‹œíŠ¸ íƒ€ì…ë³„ ë¶„ê¸° ì²˜ë¦¬
                if sheet_type == SheetType.TOC:
                    # ëª©ì°¨ ì‹œíŠ¸ - ê±´ë„ˆë›°ê¸°
                    logger.info(f"[{sheet_name}] ëª©ì°¨ ì‹œíŠ¸ì…ë‹ˆë‹¤. ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                    self.stats['skipped_sheets'] += 1
                
                elif sheet_type in [SheetType.REV_MANAGED, SheetType.VERSION_MANAGED]:
                    # REV/ì‘ì„±ë²„ì „ ê´€ë¦¬ ì‹œíŠ¸
                    self.stats['revision_sheets'] += 1
                    self.process_sheet_with_revision(sheet_name, sheet_type, items, headers)
                
                elif sheet_type == SheetType.ATTACHMENT:
                    # ì²¨ë¶€íŒŒì¼ ì‹œíŠ¸ (ê¸°ì¡´ ë°©ì‹)
                    self.stats['attachment_sheets'] += 1
                    self.process_sheet_attachments(sheet_name, items)
                
                elif sheet_type in [SheetType.HISTORY, SheetType.SOFTWARE]:
                    # ì´ë ¥ê´€ë¦¬/ì†Œí”„íŠ¸ì›¨ì–´ í˜•ìƒê¸°ë¡ ì‹œíŠ¸
                    self.stats['history_sheets'] += 1
                    self.process_sheet_as_text(sheet_name, sheet_type)
                
                elif sheet_type == SheetType.UNKNOWN:
                    # ë¯¸ë¶„ë¥˜ ì‹œíŠ¸ - ì²¨ë¶€íŒŒì¼ë¡œ ì²˜ë¦¬
                    logger.warning(f"[{sheet_name}] ë¯¸ë¶„ë¥˜ ì‹œíŠ¸ì…ë‹ˆë‹¤. ì²¨ë¶€íŒŒì¼ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                    self.stats['attachment_sheets'] += 1
                    self.process_sheet_attachments(sheet_name, items)
            
            # 5. ì„ì‹œ íŒŒì¼ ì •ë¦¬
            self.file_handler.cleanup_temp()
            
            # 6. ë³µí˜¸í™”ëœ íŒŒì¼ ì •ë¦¬
            if self.crypto_handler and self.crypto_handler.enabled:
                self.crypto_handler.cleanup_decrypted_files()
            
            # 7. í†µê³„ ì¶œë ¥
            self.print_statistics()
        
        except Exception as e:
            logger.error(f"ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if self.excel_processor:
                self.excel_processor.close()
            if self.db_processor and self.db_processor.connector:
                self.db_processor.connector.close()
            
            logger.info("="*80)
            logger.info("ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")
            logger.info("="*80)
    
    def process_sheet_with_revision(
        self, 
        sheet_name: str, 
        sheet_type: SheetType,
        items: List[Dict], 
        headers: List[str],
        monitor_progress: bool = True
    ):
        """
        Revision ê´€ë¦¬ ì‹œíŠ¸ ì²˜ë¦¬ (REV/ì‘ì„±ë²„ì „)
        
        Args:
            sheet_name: ì‹œíŠ¸ ì´ë¦„
            sheet_type: ì‹œíŠ¸ íƒ€ì…
            items: í•­ëª© ëª©ë¡ (document_key, revision í¬í•¨)
            headers: í—¤ë” ë¦¬ìŠ¤íŠ¸
            monitor_progress: íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì—¬ë¶€
        """
        if not items:
            logger.warning(f"ì‹œíŠ¸ '{sheet_name}'ì— ì²˜ë¦¬í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"[{sheet_name}] Revision ê´€ë¦¬ ì‹œíŠ¸ ì²˜ë¦¬ ì‹œì‘ (í•­ëª© ìˆ˜: {len(items)})")
        
        try:
            # ì§€ì‹ë² ì´ìŠ¤ ìƒì„±
            dataset_name = f"{sheet_name}"
            dataset_description = f"ì—‘ì…€ ì‹œíŠ¸ '{sheet_name}'ì—ì„œ ìë™ ìƒì„±ëœ ì§€ì‹ë² ì´ìŠ¤ (Revision ê´€ë¦¬)"
            
            dataset = self.ragflow_client.get_or_create_dataset(
                name=dataset_name,
                description=dataset_description,
                permission=DATASET_PERMISSION,
                embedding_model=None,
                chunk_method=CHUNK_METHOD,
                parser_config=PARSER_CONFIG
            )
            
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {sheet_name}")
                return
            
            self.stats['datasets_created'] += 1
            
            # Revision ê´€ë¦¬ê°€ í™œì„±í™”ëœ ê²½ìš°: RevisionDBì—ì„œ ê¸°ì¡´ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
            existing_docs_map = {}  # document_key -> List[{doc_id, revision, name}]
            dataset_id = dataset.get('id')
            
            if ENABLE_REVISION_MANAGEMENT:
                logger.info(f"[{sheet_name}] RevisionDBì—ì„œ ê¸°ì¡´ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘...")
                db_docs = self.revision_db.get_all_documents(dataset_id=dataset_id)
                
                # ë¬¸ì„œë¥¼ document_keyë¡œ ê·¸ë£¹í™” (í•˜ë‚˜ì˜ í‚¤ê°€ ì—¬ëŸ¬ íŒŒì¼ì„ ê°€ì§ˆ ìˆ˜ ìˆìŒ)
                for doc in db_docs:
                    doc_key = doc.get('document_key')
                    if doc_key:
                        if doc_key not in existing_docs_map:
                            existing_docs_map[doc_key] = []
                        
                        existing_docs_map[doc_key].append({
                            'doc_id': doc.get('document_id'),
                            'revision': doc.get('revision'),
                            'name': doc.get('file_name'),
                            'is_archive': doc.get('is_part_of_archive', False)
                        })
                
                total_files = sum(len(files) for files in existing_docs_map.values())
                logger.info(f"[{sheet_name}] RevisionDBì—ì„œ ê¸°ì¡´ ë¬¸ì„œ {len(existing_docs_map)}ê°œ (ì´ {total_files}ê°œ íŒŒì¼) ë°œê²¬")
            
            # ê° í•­ëª© ì²˜ë¦¬ (ì—…ë¡œë“œëœ ë¬¸ì„œ ID ìˆ˜ì§‘)
            uploaded_document_ids = []  # v21: íŒŒì‹±í•  ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
            
            for item in items:
                document_key = item.get('document_key')
                new_revision = item.get('revision')
                
                if not document_key:
                    logger.warning(f"í–‰ {item.get('row_number')}: document_keyê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # Revision ë¹„êµ ë° ì²˜ë¦¬
                if ENABLE_REVISION_MANAGEMENT and document_key in existing_docs_map:
                    existing_files = existing_docs_map[document_key]  # List[{doc_id, revision, name}] í˜¹ì€ Dict
                    # ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ ëª¨ë‘ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    files_list = existing_files if isinstance(existing_files, list) else ([existing_files] if isinstance(existing_files, dict) else [])
                    old_revision = files_list[0].get('revision') if files_list else None
                    file_count = len(files_list)
                    
                    # Revision ë¹„êµ
                    if old_revision and new_revision:
                        if old_revision == new_revision:
                            # ë™ì¼ ë²„ì „ - ê±´ë„ˆë›°ê¸°
                            if SKIP_SAME_REVISION:
                                logger.info(f"  [{document_key}] ë™ì¼ revision ({new_revision}) - ê±´ë„ˆëœ€")
                                self.stats['skipped_documents'] += 1
                                continue
                        elif not self.is_revision_newer(old_revision, new_revision):
                            # ì´ì „ ë²„ì „ - ê±´ë„ˆë›°ê¸°
                            logger.info(f"  [{document_key}] ì´ì „ revision ({new_revision} <= {old_revision}) - ê±´ë„ˆëœ€")
                            self.stats['skipped_documents'] += 1
                            continue
                        else:
                            # ì—…ë°ì´íŠ¸ í•„ìš”
                            logger.info(f"  [{document_key}] Revision ì—…ë°ì´íŠ¸: {old_revision} â†’ {new_revision}")
                            
                            # ê¸°ì¡´ ë¬¸ì„œë“¤ ì‚­ì œ (ì••ì¶• íŒŒì¼ì¸ ê²½ìš° ì—¬ëŸ¬ ê°œ)
                            if DELETE_BEFORE_UPLOAD:
                                logger.info(f"    ê¸°ì¡´ íŒŒì¼ {file_count}ê°œ ì‚­ì œ ì¤‘...")
                                deleted_count = 0
                                failed_count = 0
                                
                                for file_info in files_list:
                                    doc_id = file_info.get('doc_id')
                                    file_name = file_info.get('name')
                                    
                                    if self.ragflow_client.delete_document(dataset, doc_id):
                                        deleted_count += 1
                                        logger.debug(f"      âœ“ RAGFlow ì‚­ì œ: {file_name}")
                                    else:
                                        failed_count += 1
                                        logger.warning(f"      âœ— RAGFlow ì‚­ì œ ì‹¤íŒ¨: {file_name}")
                                
                                # RevisionDBì—ì„œë„ í•´ë‹¹ í‚¤ì˜ ëª¨ë“  íŒŒì¼ ì‚­ì œ
                                db_deleted = self.revision_db.delete_document(document_key, dataset_id)
                                
                                self.stats['deleted_documents'] += deleted_count
                                self.stats['failed_deletions'] += failed_count
                                
                                if deleted_count > 0:
                                    logger.info(f"    âœ“ ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {deleted_count}ê°œ (ì‹¤íŒ¨: {failed_count}ê°œ)")
                                
                                if failed_count == file_count:
                                    logger.error(f"    âœ— ëª¨ë“  ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                                    continue
                    else:
                        logger.debug(f"  [{document_key}] Revision ì •ë³´ ë¶ˆì™„ì „ - ì—…ë°ì´íŠ¸ ì§„í–‰")
                    
                    # íŒŒì¼ ì—…ë¡œë“œ (v21: ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
                    doc_ids = self.process_item(dataset, item)
                    if doc_ids:
                        uploaded_document_ids.extend(doc_ids)
                        self.stats['updated_documents'] += 1
                        logger.info(f"    âœ“ ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì™„ë£Œ ({len(doc_ids)}ê°œ íŒŒì¼)")
                
                else:
                    # ì‹ ê·œ ë¬¸ì„œ
                    logger.info(f"  [{document_key}] ì‹ ê·œ ë¬¸ì„œ (revision: {new_revision})")
                    doc_ids = self.process_item(dataset, item)
                    if doc_ids:
                        uploaded_document_ids.extend(doc_ids)
                        self.stats['new_documents'] += 1
                        logger.info(f"    âœ“ ì‹ ê·œ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ ({len(doc_ids)}ê°œ íŒŒì¼)")
            
            # v21: ì—…ë¡œë“œëœ ë¬¸ì„œ IDë“¤ë§Œ íŒŒì‹±
            if uploaded_document_ids:
                if AUTO_PARSE_AFTER_UPLOAD:
                    logger.info(f"[{sheet_name}] {len(uploaded_document_ids)}ê°œ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ, íŒŒì‹± ì‹œì‘")
                    parse_started = self.ragflow_client.start_batch_parse(
                        dataset,
                        document_ids=uploaded_document_ids
                    )
                    
                    if parse_started and monitor_progress and MONITOR_PARSE_PROGRESS:
                        self.monitor_parse_progress(dataset, sheet_name, uploaded_document_ids, max_wait_minutes=PARSE_TIMEOUT_MINUTES)
                    elif parse_started:
                        logger.info(f"[{sheet_name}] íŒŒì‹±ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
                else:
                    logger.info(f"[{sheet_name}] {len(uploaded_document_ids)}ê°œ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ (ìë™ íŒŒì‹± ë¹„í™œì„±í™”)")
            else:
                logger.info(f"[{sheet_name}] ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.info(f"[{sheet_name}] Revision ê´€ë¦¬ ì‹œíŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
        
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def process_sheet_attachments(self, sheet_name: str, items: List[Dict], monitor_progress: bool = False):
        """
        ì²¨ë¶€íŒŒì¼ ì‹œíŠ¸ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹ - Revision ê´€ë¦¬ ì—†ìŒ)
        
        Args:
            sheet_name: ì‹œíŠ¸ ì´ë¦„
            items: í•˜ì´í¼ë§í¬ì™€ ë©”íƒ€ë°ì´í„° ëª©ë¡
            monitor_progress: íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì—¬ë¶€
        """
        if not items:
            logger.warning(f"ì‹œíŠ¸ '{sheet_name}'ì— ì²˜ë¦¬í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"[{sheet_name}] ì²¨ë¶€íŒŒì¼ ì‹œíŠ¸ ì²˜ë¦¬ ì‹œì‘ (í•­ëª© ìˆ˜: {len(items)})")
        
        try:
            # ì§€ì‹ë² ì´ìŠ¤ ìƒì„±
            dataset_name = f"{sheet_name}"
            dataset_description = f"ì—‘ì…€ ì‹œíŠ¸ '{sheet_name}'ì—ì„œ ìë™ ìƒì„±ëœ ì§€ì‹ë² ì´ìŠ¤"
            
            dataset = self.ragflow_client.get_or_create_dataset(
                name=dataset_name,
                description=dataset_description,
                permission=DATASET_PERMISSION,
                embedding_model=None,
                chunk_method=CHUNK_METHOD,
                parser_config=PARSER_CONFIG
            )
            
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {sheet_name}")
                return
            
            self.stats['datasets_created'] += 1
            
            # ê° í•­ëª© ì²˜ë¦¬ (v21: ë¬¸ì„œ ID ìˆ˜ì§‘)
            uploaded_document_ids = []
            for item in items:
                doc_ids = self.process_item(dataset, item, check_processed_urls=True)
                if doc_ids:
                    uploaded_document_ids.extend(doc_ids)
            
            # v21: ì—…ë¡œë“œëœ ë¬¸ì„œ IDë“¤ë§Œ íŒŒì‹±
            if uploaded_document_ids:
                if AUTO_PARSE_AFTER_UPLOAD:
                    logger.info(f"[{sheet_name}] {len(uploaded_document_ids)}ê°œ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ, íŒŒì‹± ì‹œì‘")
                    parse_started = self.ragflow_client.start_batch_parse(
                        dataset,
                        document_ids=uploaded_document_ids
                    )
                    
                    if parse_started and monitor_progress and MONITOR_PARSE_PROGRESS:
                        self.monitor_parse_progress(dataset, sheet_name, uploaded_document_ids, max_wait_minutes=PARSE_TIMEOUT_MINUTES)
                    elif parse_started:
                        logger.info(f"[{sheet_name}] íŒŒì‹±ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
                else:
                    logger.info(f"[{sheet_name}] {len(uploaded_document_ids)}ê°œ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ (ìë™ íŒŒì‹± ë¹„í™œì„±í™”)")
            
            logger.info(f"[{sheet_name}] ì²¨ë¶€íŒŒì¼ ì‹œíŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
        
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def process_sheet_as_text(self, sheet_name: str, sheet_type: SheetType, monitor_progress: bool = False):
        """
        ì´ë ¥ê´€ë¦¬/ì†Œí”„íŠ¸ì›¨ì–´ í˜•ìƒê¸°ë¡ ì‹œíŠ¸ë¥¼ í…ìŠ¤íŠ¸ ë˜ëŠ” Excelë¡œ ë³€í™˜í•˜ì—¬ ì—…ë¡œë“œ
        
        Args:
            sheet_name: ì‹œíŠ¸ ì´ë¦„
            sheet_type: ì‹œíŠ¸ íƒ€ì… (HISTORY ë˜ëŠ” SOFTWARE)
            monitor_progress: íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì—¬ë¶€
        """
        upload_format = HISTORY_SHEET_UPLOAD_FORMAT
        logger.info(f"[{sheet_name}] ì‹œíŠ¸ ì²˜ë¦¬ ì‹œì‘ (í˜•ì‹: {upload_format.upper()})")
        
        try:
            # ì§€ì‹ë² ì´ìŠ¤ ìƒì„±
            dataset_name = f"{sheet_name}"
            dataset_description = f"ì—‘ì…€ ì‹œíŠ¸ '{sheet_name}' ({sheet_type.value})"
            
            dataset = self.ragflow_client.get_or_create_dataset(
                name=dataset_name,
                description=dataset_description,
                permission=DATASET_PERMISSION,
                embedding_model=None,
                chunk_method=CHUNK_METHOD,
                parser_config=PARSER_CONFIG
            )
            
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {sheet_name}")
                return
            
            self.stats['datasets_created'] += 1
            
            # ì—…ë¡œë“œ ì „ ì „ëŸ‰ ì‚­ì œ(ë¬¸ì„œ+ì—°ê²° íŒŒì¼) - íˆìŠ¤í† ë¦¬/ì†Œí”„íŠ¸ì›¨ì–´ ì‹œíŠ¸ ì „ìš© í¼ì§€
            if PURGE_BEFORE_HISTORY_SOFTWARE:
                try:
                    logger.info(f"[{sheet_name}] ì—…ë¡œë“œ ì „ ë°ì´í„°ì…‹ ì „ëŸ‰ ì‚­ì œ(ë¬¸ì„œ+íŒŒì¼) ìˆ˜í–‰")
                    purge_result = self.ragflow_client.delete_all_documents_and_files_in_dataset(dataset)
                    logger.info(
                        f"[{sheet_name}] í¼ì§€ ê²°ê³¼ - ë¬¸ì„œ: {purge_result.get('deleted_documents', 0)} ì‚­ì œ "
                        f"(ì‹¤íŒ¨ {purge_result.get('failed_documents', 0)}) | "
                        f"íŒŒì¼: {purge_result.get('deleted_files', 0)} ì‚­ì œ "
                        f"(ì‹¤íŒ¨ {purge_result.get('failed_files', 0)})"
                    )
                except Exception as e:
                    logger.error(f"[{sheet_name}] í¼ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
                
            # v21: ì—…ë¡œë“œëœ ë¬¸ì„œ ID ì¶”ì 
            uploaded_document_ids = []
            
            if upload_format == "excel":
                # Excel íŒŒì¼ë¡œ ì¶”ì¶œí•˜ì—¬ ì—…ë¡œë“œ
                logger.info(f"[{sheet_name}] Excel íŒŒì¼ë¡œ ì¶”ì¶œ ì¤‘...")
                excel_file_path = self.excel_processor.extract_sheet_as_excel(sheet_name, TEMP_DIR)
                
                if not excel_file_path:
                    logger.error(f"[{sheet_name}] Excel ì¶”ì¶œ ì‹¤íŒ¨")
                    return
                
                # Excel íŒŒì¼ ì—…ë¡œë“œ
                metadata = {
                    'ì‹œíŠ¸ëª…': sheet_name,
                    'íƒ€ì…': sheet_type.value,
                    'íŒŒì¼í˜•ì‹': 'excel'
                }
                
                upload_result = self.ragflow_client.upload_document(
                    dataset=dataset,
                    file_path=excel_file_path,
                    metadata=metadata,
                    display_name=f"{sheet_name}.xlsx"
                )
                
                if upload_result:
                    doc_id = upload_result.get('document_id')
                    uploaded_document_ids.append(doc_id)
                    self.stats['successful_uploads'] += 1
                    logger.info(f"[{sheet_name}] Excel íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")
                else:
                    self.stats['failed_uploads'] += 1
                    logger.error(f"[{sheet_name}] Excel íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨")
            
            else:  # upload_format == "text" - PDFë¡œ ë³€í™˜í•˜ì—¬ ì—…ë¡œë“œ
                # í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ PDFë¡œ ë³€í™˜ (ì—¬ëŸ¬ ì²­í¬ ê°€ëŠ¥)
                logger.info(f"[{sheet_name}] í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...")
                # PDF ë³€í™˜ ì‹œ Row ë‹¨ìœ„ í˜ì´ì§€ ì œì–´ë¥¼ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜ë°›ìŒ
                text_chunks = self.excel_processor.convert_sheet_to_text_chunks(
                    sheet_name,
                    return_rows_as_list=True
                )
                
                if not text_chunks:
                    logger.warning(f"[{sheet_name}] ë³€í™˜ëœ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    return
                
                logger.info(f"[{sheet_name}] {len(text_chunks)}ê°œ ì²­í¬ ìƒì„±ë¨")
                
                # ê° ì²­í¬ë¥¼ PDFë¡œ ë³€í™˜í•˜ì—¬ ì—…ë¡œë“œ
                for chunk_idx, chunk_content in enumerate(text_chunks, 1):
                    # íŒŒì¼ëª…: ì²­í¬ê°€ 1ê°œë©´ ë²ˆí˜¸ ì—†ì´, ì—¬ëŸ¬ ê°œë©´ ë²ˆí˜¸ ë¶™ì„
                    if len(text_chunks) == 1:
                        filename = f"{sheet_name}_{sheet_type.value}"
                        display_name = f"{sheet_name}_{sheet_type.value}.pdf"
                    else:
                        filename = f"{sheet_name}_{sheet_type.value}_part{chunk_idx}"
                        display_name = f"{sheet_name}_{sheet_type.value}_part{chunk_idx}.pdf"
                    
                    # í…ìŠ¤íŠ¸ë¥¼ PDFë¡œ ë³€í™˜
                    pdf_file_path = self.file_handler.convert_text_to_pdf(chunk_content, filename)
                    
                    if not pdf_file_path:
                        logger.error(f"[{sheet_name}] ì²­í¬ {chunk_idx} PDF ë³€í™˜ ì‹¤íŒ¨")
                        self.stats['failed_uploads'] += 1
                        continue
                    
                    # PDF íŒŒì¼ ì—…ë¡œë“œ
                    metadata = {
                        'ì‹œíŠ¸ëª…': sheet_name,
                        'íƒ€ì…': sheet_type.value,
                        'íŒŒì¼í˜•ì‹': 'pdf',
                        'ì²­í¬_ë²ˆí˜¸': str(chunk_idx) if len(text_chunks) > 1 else '1',
                        'ì´_ì²­í¬_ìˆ˜': str(len(text_chunks))
                    }
                    
                    upload_result = self.ragflow_client.upload_document(
                        dataset=dataset,
                        file_path=pdf_file_path,
                        metadata=metadata,
                        display_name=display_name
                    )
                    
                    if upload_result:
                        doc_id = upload_result.get('document_id')
                        uploaded_document_ids.append(doc_id)
                        self.stats['successful_uploads'] += 1
                        logger.info(f"[{sheet_name}] ì²­í¬ {chunk_idx}/{len(text_chunks)} PDF ì—…ë¡œë“œ ì™„ë£Œ")
                    else:
                        self.stats['failed_uploads'] += 1
                        logger.error(f"[{sheet_name}] ì²­í¬ {chunk_idx}/{len(text_chunks)} PDF ì—…ë¡œë“œ ì‹¤íŒ¨")
            
            # v21: ì—…ë¡œë“œëœ ë¬¸ì„œ IDë“¤ë§Œ íŒŒì‹±
            if uploaded_document_ids:
                if AUTO_PARSE_AFTER_UPLOAD:
                    logger.info(f"[{sheet_name}] {len(uploaded_document_ids)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ, íŒŒì‹± ì‹œì‘")
                    parse_started = self.ragflow_client.start_batch_parse(
                        dataset,
                        document_ids=uploaded_document_ids
                    )
                    
                    if parse_started and monitor_progress and MONITOR_PARSE_PROGRESS:
                        self.monitor_parse_progress(dataset, sheet_name, uploaded_document_ids, max_wait_minutes=PARSE_TIMEOUT_MINUTES)
                    elif parse_started:
                        logger.info(f"[{sheet_name}] íŒŒì‹±ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
                else:
                    logger.info(f"[{sheet_name}] {len(uploaded_document_ids)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ (ìë™ íŒŒì‹± ë¹„í™œì„±í™”)")
            
            logger.info(f"[{sheet_name}] ì‹œíŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
        
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def process_sheet(self, sheet_name: str, items: List[Dict], monitor_progress: bool = False):
        """
        ì‹œíŠ¸ ë‹¨ìœ„ ì²˜ë¦¬
        
        Args:
            sheet_name: ì‹œíŠ¸ ì´ë¦„
            items: í•˜ì´í¼ë§í¬ì™€ ë©”íƒ€ë°ì´í„° ëª©ë¡
            monitor_progress: íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì—¬ë¶€ (ê¸°ë³¸: False)
        """
        if not items:
            logger.warning(f"ì‹œíŠ¸ '{sheet_name}'ì— ì²˜ë¦¬í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.log_sheet_start(sheet_name)
        
        try:
            # ì‹œíŠ¸ë³„ ì§€ì‹ë² ì´ìŠ¤ ìƒì„±
            dataset_name = f"{sheet_name}"
            dataset_description = f"ì—‘ì…€ ì‹œíŠ¸ '{sheet_name}'ì—ì„œ ìë™ ìƒì„±ëœ ì§€ì‹ë² ì´ìŠ¤"
            
            dataset = self.ragflow_client.get_or_create_dataset(
                name=dataset_name,
                description=dataset_description,
                permission=DATASET_PERMISSION,
                embedding_model=None,  # ì‹œìŠ¤í…œ ê¸°ë³¸ê°’ ì‚¬ìš© (tenant.embd_id)
                chunk_method=CHUNK_METHOD,  # GUIì™€ ë™ì¼í•œ íŒŒì‹± ë°©ë²•
                parser_config=PARSER_CONFIG  # GUIì™€ ë™ì¼í•œ íŒŒì„œ ì„¤ì •
            )
            
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {sheet_name}")
                return
            
            self.stats['datasets_created'] += 1
            
            # ê° í•­ëª© ì²˜ë¦¬
            uploaded_count = 0
            for item in items:
                if self.process_item(dataset, item):
                    uploaded_count += 1
            
            # ì¼ê´„ íŒŒì‹± ì‹œì‘
            if uploaded_count > 0:
                if AUTO_PARSE_AFTER_UPLOAD:
                    logger.info(f"ì‹œíŠ¸ '{sheet_name}': {uploaded_count}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ, ì¼ê´„ íŒŒì‹± ì‹œì‘")
                    parse_started = self.ragflow_client.start_batch_parse(dataset)
                    
                    # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ (ì˜µì…˜)
                    if parse_started and monitor_progress:
                        self.monitor_parse_progress(dataset, sheet_name, max_wait_minutes=PARSE_TIMEOUT_MINUTES)
                    elif parse_started:
                        logger.info(f"ì‹œíŠ¸ '{sheet_name}': íŒŒì‹±ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤. Management UIì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
                else:
                    logger.info(f"ì‹œíŠ¸ '{sheet_name}': {uploaded_count}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ (ìë™ íŒŒì‹± ë¹„í™œì„±í™”)")
            
            logger.log_sheet_end(sheet_name, uploaded_count)
        
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def process_item(self, dataset: object, item: Dict, check_processed_urls: bool = False) -> List[str]:
        """
        ê°œë³„ í•­ëª© ì²˜ë¦¬ (íŒŒì¼ ë‹¤ìš´ë¡œë“œ, ë³€í™˜, ì—…ë¡œë“œ)
        
        Args:
            dataset: Dataset ê°ì²´
            item: {'hyperlink': '...', 'metadata': {...}, 'document_key': '...', 'revision': '...', ...}
            check_processed_urls: ì´ë¯¸ ì²˜ë¦¬ëœ URLì¸ì§€ í™•ì¸í• ì§€ ì—¬ë¶€ (Revision ê´€ë¦¬ ì•ˆí•˜ëŠ” ì‹œíŠ¸ìš©)
        
        Returns:
            ì—…ë¡œë“œëœ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸ (ì„±ê³µ ì‹œ) ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ)
        """
        hyperlinks = []
        # hyperlinks ë°°ì—´ ìš°ì„ , ì—†ìœ¼ë©´ ë‹¨ì¼ hyperlink ì‚¬ìš©
        if isinstance(item.get('hyperlinks'), list) and item.get('hyperlinks'):
            hyperlinks = [h for h in item.get('hyperlinks') if isinstance(h, str) and h.strip()]
        elif item.get('hyperlink'):
            hyperlinks = [item.get('hyperlink')]
        metadata = item.get('metadata', {})
        row_number = item.get('row_number')
        document_key = item.get('document_key')
        revision = item.get('revision')
        
        if not hyperlinks:
            logger.warning(f"{row_number}í–‰: í•˜ì´í¼ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        all_uploaded_doc_ids: List[str] = []
        for hyperlink in hyperlinks:
            # ì²˜ë¦¬ëœ URL í™•ì¸ (Revision ê´€ë¦¬ ì•ˆí•˜ëŠ” ì‹œíŠ¸ìš©)
            if check_processed_urls and self.revision_db.is_url_processed(hyperlink):
                logger.info(f"{row_number}í–‰: ì´ë¯¸ ì²˜ë¦¬ëœ URLì´ë¯€ë¡œ ìŠ¤í‚µí•©ë‹ˆë‹¤ - {hyperlink}")
                continue

            self.stats['total_files'] += 1
            try:
                # 1. íŒŒì¼ ê°€ì ¸ì˜¤ê¸° (ë‹¤ìš´ë¡œë“œ ë˜ëŠ” ë³µì‚¬)
                file_path = self.file_handler.get_file(hyperlink)
                
                if not file_path:
                    logger.error(f"{row_number}í–‰: íŒŒì¼ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ - {hyperlink}")
                    self.stats['failed_uploads'] += 1
                    continue
                
                # 2. íŒŒì¼ ì²˜ë¦¬ (í˜•ì‹ ë³€í™˜)
                processed_files = self.file_handler.process_file(file_path)
                
                if not processed_files:
                    logger.error(f"{row_number}í–‰: íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ - {file_path.name}")
                    self.stats['failed_uploads'] += 1
                    continue
                
                # 3. ì²˜ë¦¬ëœ íŒŒì¼ë“¤ì„ RAGFlowì— ì—…ë¡œë“œ
                # ì••ì¶• íŒŒì¼ ì—¬ë¶€ í™•ì¸ (ZIP íŒŒì¼ì´ê³  ì—¬ëŸ¬ íŒŒì¼ì´ ì¶”ì¶œëœ ê²½ìš°)
                is_archive = file_path.suffix.lower() == '.zip' and len(processed_files) > 1
                archive_source = file_path.name if is_archive else None
                
                if is_archive:
                    logger.info(f"ì••ì¶• íŒŒì¼ ê°ì§€: {file_path.name} ({len(processed_files)}ê°œ íŒŒì¼ ì¶”ì¶œë¨)")
                
                for processed_path, file_type in processed_files:
                    # ë©”íƒ€ë°ì´í„°ì— ì›ë³¸ ì •ë³´ ì¶”ê°€
                    enhanced_metadata = metadata.copy()
                    enhanced_metadata['ì›ë³¸_íŒŒì¼'] = file_path.name
                    enhanced_metadata['íŒŒì¼_í˜•ì‹'] = file_type
                    enhanced_metadata['ì—‘ì…€_í–‰ë²ˆí˜¸'] = str(row_number)
                    enhanced_metadata['í•˜ì´í¼ë§í¬'] = hyperlink
                    
                    # ì••ì¶• íŒŒì¼ ì •ë³´ ì¶”ê°€
                    if is_archive:
                        enhanced_metadata['ì••ì¶•íŒŒì¼'] = archive_source
                        enhanced_metadata['ì••ì¶•íŒŒì¼_ë‚´_íŒŒì¼ëª…'] = processed_path.name
                    
                    # Revision ê´€ë¦¬ ì •ë³´ ì¶”ê°€
                    if document_key:
                        enhanced_metadata['document_key'] = document_key
                    if revision:
                        enhanced_metadata['revision'] = revision
                    
                    # ì—…ë¡œë“œ (document_id ë° file_id ë°˜í™˜)
                    upload_result = self.ragflow_client.upload_document(
                        dataset=dataset,
                        file_path=processed_path,
                        metadata=enhanced_metadata,
                        display_name=processed_path.name
                    )
                    
                    if upload_result:
                        doc_id = upload_result.get('document_id')
                        file_id = upload_result.get('file_id')

                        # Excel íŒŒì¼ì¸ ê²½ìš° chunk_methodë¥¼ "table"ë¡œ ì„¤ì •
                        if file_type in ['xlsx', 'xls', 'xlsm']:
                            self.ragflow_client.update_document_parser(
                                dataset_id=dataset.get('id'),
                                document_id=doc_id,
                                chunk_method="table"
                            )

                        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ì—…ë¡œë“œ í›„ ë³„ë„ í˜¸ì¶œ)
                        # ì¤‘ìš”: ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì—‘ì…€ì˜ rowë³„ í—¤ë”:ê°’(metadata)ë§Œ ì „ë‹¬í•œë‹¤.
                        self.ragflow_client.update_document(dataset.get('id'), doc_id, metadata)

                        all_uploaded_doc_ids.append(doc_id)
                        self.stats['successful_uploads'] += 1
                        logger.log_file_process(
                            processed_path.name, 
                            "ì—…ë¡œë“œ ì„±ê³µ",
                            f"í˜•ì‹: {file_type}, í–‰: {row_number}, ë¬¸ì„œID: {doc_id}, íŒŒì¼ID: {file_id}"
                        )
                        
                        # RevisionDBì— ì €ì¥ (revision ê´€ë¦¬ê°€ í™œì„±í™”ëœ ê²½ìš°)
                        if ENABLE_REVISION_MANAGEMENT and document_key:
                            dataset_id = dataset.get('id')
                            dataset_name = dataset.get('name')
                            
                            # DB ì €ì¥ ì‹œë„
                            db_success = self.revision_db.save_document(
                                document_key=document_key,
                                document_id=doc_id,
                                file_id=file_id,
                                dataset_id=dataset_id,
                                dataset_name=dataset_name,
                                revision=revision,
                                file_path=str(processed_path),
                                file_name=processed_path.name,
                                is_part_of_archive=is_archive,
                                archive_source=archive_source
                            )
                            
                            if db_success:
                                if is_archive:
                                    logger.debug(f"RevisionDBì— ì €ì¥ (ì••ì¶• íŒŒì¼): {document_key}/{processed_path.name} â†’ {doc_id} (íŒŒì¼ID: {file_id})")
                                else:
                                    logger.debug(f"RevisionDBì— ì €ì¥: {document_key} â†’ {doc_id} (íŒŒì¼ID: {file_id})")
                            else:
                                # DB ì €ì¥ ì‹¤íŒ¨ ì‹œ RAGFlow ì—…ë¡œë“œ ë¡¤ë°± (ì‚­ì œ)
                                logger.error(f"RevisionDB ì €ì¥ ì‹¤íŒ¨! ë°ì´í„° ì •í•©ì„±ì„ ìœ„í•´ RAGFlow ì—…ë¡œë“œë¥¼ ë¡¤ë°±(ì‚­ì œ)í•©ë‹ˆë‹¤: {processed_path.name}")
                                try:
                                    self.ragflow_client.delete_document(dataset, doc_id)
                                    logger.info(f"  âœ“ ë¡¤ë°± ì„±ê³µ: ë¬¸ì„œ ì‚­ì œë¨ ({doc_id})")
                                except Exception as e:
                                    logger.error(f"  âœ— ë¡¤ë°± ì‹¤íŒ¨: ë¬¸ì„œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‚­ì œí•´ì•¼ í•©ë‹ˆë‹¤ ({doc_id}): {e}")
                                
                                # ì—…ë¡œë“œ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬ ë° í†µê³„ ìˆ˜ì •
                                if doc_id in all_uploaded_doc_ids:
                                    all_uploaded_doc_ids.remove(doc_id)
                                self.stats['successful_uploads'] -= 1
                                self.stats['failed_uploads'] += 1
                                continue  # ë‹¤ìŒ íŒŒì¼ ì²˜ë¦¬
                        
                        # ì²˜ë¦¬ëœ URL ì €ì¥ (Revision ê´€ë¦¬ ì•ˆí•˜ëŠ” ì‹œíŠ¸ìš©)
                        if check_processed_urls:
                            self.revision_db.add_processed_url(hyperlink)

                    else:
                        self.stats['failed_uploads'] += 1
                        logger.log_file_process(
                            processed_path.name, 
                            "ì—…ë¡œë“œ ì‹¤íŒ¨",
                            f"í˜•ì‹: {file_type}, í–‰: {row_number}"
                        )
            except Exception as e:
                logger.error(f"{row_number}í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                self.stats['failed_uploads'] += 1
                continue
        
        return all_uploaded_doc_ids
    
    def monitor_parse_progress(self, dataset: Dict, dataset_name: str, document_ids: List[str] = None, max_wait_minutes: int = 30):
        """
        íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ (RAGFlow v21)
        
        Args:
            dataset: Dataset ë”•ì…”ë„ˆë¦¬
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ (ë¡œê·¸ìš©)
            document_ids: ëª¨ë‹ˆí„°ë§í•  ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
            max_wait_minutes: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ë¶„, ê¸°ë³¸: 30ë¶„)
        """
        logger.info(f"[{dataset_name}] ğŸ“Š íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
        if document_ids:
            logger.info(f"[{dataset_name}] ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ: {len(document_ids)}ê°œ ë¬¸ì„œ")
        logger.info(f"[{dataset_name}] ìµœëŒ€ ëŒ€ê¸° ì‹œê°„: {max_wait_minutes}ë¶„")
        
        start_time = time.time()
        max_wait_seconds = max_wait_minutes * 60
        check_interval = 10  # 10ì´ˆë§ˆë‹¤ í™•ì¸
        last_status = None
        
        while True:
            try:
                # v21: ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸ë¡œ ì§„í–‰ ìƒí™© ì¡°íšŒ
                progress = self.ragflow_client.get_parse_progress(dataset, document_ids)
                
                if progress:
                    status = progress.get('status', 'unknown')
                    current = progress.get('current_document_index', 0)
                    total = progress.get('total_documents', 0)
                    current_doc = progress.get('current_document_name', 'N/A')
                    
                    # ìƒíƒœ ë³€ê²½ ì‹œì—ë§Œ ë¡œê·¸ ì¶œë ¥ (ì¤‘ë³µ ë°©ì§€)
                    if status != last_status or current != getattr(self, '_last_current', -1):
                        if total > 0:
                            progress_percent = (current / total) * 100
                            logger.info(
                                f"[{dataset_name}] ğŸ“„ ì§„í–‰: {current}/{total} ({progress_percent:.1f}%) "
                                f"| ìƒíƒœ: {status} | í˜„ì¬: {current_doc}"
                            )
                        else:
                            logger.info(f"[{dataset_name}] ìƒíƒœ: {status}")
                        
                        last_status = status
                        self._last_current = current
                    
                    # ì™„ë£Œ ì²´í¬
                    if status == 'completed' or (total > 0 and current >= total):
                        logger.info(f"[{dataset_name}] âœ“ íŒŒì‹± ì™„ë£Œ!")
                        logger.info(f"[{dataset_name}] ì´ {total}ê°œ ë¬¸ì„œ íŒŒì‹± ì™„ë£Œ")
                        break
                    
                    elif status == 'error':
                        error_msg = progress.get('error_message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                        logger.error(f"[{dataset_name}] âœ— íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
                        break
                    
                    elif status == 'idle' and current == 0:
                        logger.warning(f"[{dataset_name}] âš ï¸ íŒŒì‹±ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                else:
                    logger.debug(f"[{dataset_name}] ì§„í–‰ ìƒí™© ì •ë³´ ì—†ìŒ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ëŒ€ê¸° ì¤‘...)")
                
                # íƒ€ì„ì•„ì›ƒ ì²´í¬
                elapsed = time.time() - start_time
                if elapsed > max_wait_seconds:
                    logger.warning(f"[{dataset_name}] â±ï¸ íŒŒì‹± ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ ({max_wait_minutes}ë¶„)")
                    logger.info(f"[{dataset_name}] íŒŒì‹±ì€ ê³„ì† ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. Management UIì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
                    break
                
                # ëŒ€ê¸°
                time.sleep(check_interval)
            
            except Exception as e:
                logger.error(f"[{dataset_name}] ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                logger.info(f"[{dataset_name}] Management UIì—ì„œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.")
                break
        
        # ìµœì¢… ìƒíƒœ í™•ì¸
        try:
            final_progress = self.ragflow_client.get_parse_progress(dataset)
            if final_progress:
                final_status = final_progress.get('status', 'unknown')
                logger.info(f"[{dataset_name}] ìµœì¢… ìƒíƒœ: {final_status}")
        except:
            pass
    
    def delete_knowledge_by_dataset_name(self, dataset_name: str, confirm: bool = False) -> Dict:
        """
        dataset_nameìœ¼ë¡œ RAGFlow ì§€ì‹ë² ì´ìŠ¤ì˜ ëª¨ë“  ë¬¸ì„œì™€ íŒŒì¼ì„ ì‚­ì œ
        
        Args:
            dataset_name: ì§€ì‹ë² ì´ìŠ¤ ì´ë¦„
            confirm: Trueë¡œ ì„¤ì •í•´ì•¼ë§Œ ì‹¤í–‰ë¨ (ì‹¤ìˆ˜ ë°©ì§€)
        
        Returns:
            ì‚­ì œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("="*80)
        logger.info(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}' ì „ëŸ‰ ì‚­ì œ(ë¬¸ì„œ+íŒŒì¼) ì¡°íšŒ")
        logger.info("="*80)
        
        try:
            # 1. ì§€ì‹ë² ì´ìŠ¤ ì¡°íšŒ
            logger.info(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}' ì¡°íšŒ ì¤‘...")
            dataset = self.ragflow_client.get_dataset_by_name(dataset_name)
            
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    'success': False,
                    'message': f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            dataset_id = dataset.get('id')
            logger.info(f"âœ“ ì§€ì‹ë² ì´ìŠ¤ ë°œê²¬ (dataset_id: {dataset_id})")
            
            # 2. ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
            logger.info("ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘...")
            all_documents = []
            page = 1
            page_size = 100
            while True:
                documents = self.ragflow_client.get_documents_in_dataset(dataset, page=page, page_size=page_size)
                if not documents:
                    break
                all_documents.extend(documents)
                if len(documents) < page_size:
                    break
                page += 1
            
            total_docs = len(all_documents)
            logger.info(f"âœ“ {total_docs}ê°œ ë¬¸ì„œ ë°œê²¬")
            
            if not confirm:
                # í™•ì¸ ëª¨ë“œ: ì‚­ì œí•  í•­ëª©ë§Œ ë³´ì—¬ì¤Œ
                logger.info("\nì‚­ì œ ëŒ€ìƒ í•­ëª©:")
                logger.info(f"  - ì§€ì‹ë² ì´ìŠ¤: {dataset_name} (ID: {dataset_id})")
                logger.info(f"  - ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ")
                logger.info(f"  - ì—°ê²°ëœ íŒŒì¼: {total_docs}ê°œ (ë¬¸ì„œë‹¹ 1ê°œ)")
                
                return {
                    'success': True,
                    'total_documents': total_docs,
                    'dataset_id': dataset_id,
                    'dataset_name': dataset_name
                }
            
            # 3. ì‹¤ì œ ì‚­ì œ ìˆ˜í–‰
            logger.info("\n="*80)
            logger.info(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}' ì „ëŸ‰ ì‚­ì œ ì‹œì‘")
            logger.info("="*80)
            
            purge_result = self.ragflow_client.delete_all_documents_and_files_in_dataset(dataset)
            
            deleted_docs = purge_result.get('deleted_documents', 0)
            failed_docs = purge_result.get('failed_documents', 0)
            deleted_files = purge_result.get('deleted_files', 0)
            failed_files = purge_result.get('failed_files', 0)
            
            logger.info(f"\nì‚­ì œ ê²°ê³¼:")
            logger.info(f"  - ë¬¸ì„œ: {deleted_docs}ê°œ ì‚­ì œ (ì‹¤íŒ¨: {failed_docs}ê°œ)")
            logger.info(f"  - íŒŒì¼: {deleted_files}ê°œ ì‚­ì œ (ì‹¤íŒ¨: {failed_files}ê°œ)")
            
            # 4. RevisionDBì—ì„œë„ í•´ë‹¹ datasetì˜ ëª¨ë“  í•­ëª© ì‚­ì œ
            logger.info(f"\nRevisionDBì—ì„œ '{dataset_name}' í•­ëª© ì‚­ì œ ì¤‘...")
            db_documents = self.revision_db.get_documents_by_dataset_name(dataset_name)
            db_deleted = 0
            
            if db_documents:
                for doc in db_documents:
                    document_key = doc.get('document_key')
                    file_name = doc.get('file_name', 'Unknown')
                    deleted_count = self.revision_db.delete_document(
                        document_key=document_key,
                        dataset_id=dataset_id,
                        file_name=file_name
                    )
                    if deleted_count > 0:
                        db_deleted += deleted_count
                
                logger.info(f"âœ“ RevisionDBì—ì„œ {db_deleted}ê°œ í•­ëª© ì‚­ì œ")
            else:
                logger.info("RevisionDBì— ì‚­ì œí•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            return {
                'success': True,
                'dataset_name': dataset_name,
                'dataset_id': dataset_id,
                'total_documents': total_docs,
                'deleted_documents': deleted_docs,
                'failed_documents': failed_docs,
                'deleted_files': deleted_files,
                'failed_files': failed_files,
                'db_deleted': db_deleted
            }
            
        except Exception as e:
            logger.error(f"ì§€ì‹ë² ì´ìŠ¤ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'message': str(e)
            }
    
    def delete_documents_by_dataset_name(self, dataset_name: str, confirm: bool = False) -> Dict:
        """
        dataset_nameìœ¼ë¡œ RAGFlowì™€ RevisionDBì—ì„œ ëª¨ë“  ë¬¸ì„œ ì‚­ì œ
        
        Args:
            dataset_name: ì§€ì‹ë² ì´ìŠ¤ ì´ë¦„
            confirm: Trueë¡œ ì„¤ì •í•´ì•¼ë§Œ ì‹¤í–‰ë¨ (ì‹¤ìˆ˜ ë°©ì§€)
        
        Returns:
            ì‚­ì œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not confirm:
            logger.warning("âš ï¸ ì‚­ì œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ confirm=Trueë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.")
            return {
                'success': False,
                'message': 'confirm=True í•„ìš”'
            }
        
        logger.info("="*80)
        logger.info(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}' ë¬¸ì„œ ì‚­ì œ ì‹œì‘")
        logger.info("="*80)
        
        try:
            # 1. RevisionDBì—ì„œ ë¬¸ì„œ ì¡°íšŒ
            logger.info(f"[1/2] RevisionDBì—ì„œ '{dataset_name}' ë¬¸ì„œ ì¡°íšŒ ì¤‘...")
            documents = self.revision_db.get_documents_by_dataset_name(dataset_name)
            
            if not documents:
                logger.warning(f"'{dataset_name}'ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    'success': True,
                    'total_documents': 0,
                    'ragflow_deleted': 0,
                    'ragflow_failed': 0,
                    'db_deleted': 0
                }
            
            total_docs = len(documents)
            dataset_id = documents[0].get('dataset_id')
            logger.info(f"âœ“ {total_docs}ê°œ ë¬¸ì„œ ë°œê²¬ (dataset_id: {dataset_id})")
            
            # 2. RAGFlow ë° DBì—ì„œ ìˆœì°¨ ì‚­ì œ (ì„±ê³µ ì‹œì—ë§Œ DB ì‚­ì œ)
            logger.info(f"\n[2/2] RAGFlow ë° DBì—ì„œ ë¬¸ì„œ ì‚­ì œ ì¤‘...")
            ragflow_deleted = 0
            ragflow_failed = 0
            db_deleted = 0
            failed_items = []
            
            # dataset ì •ë³´ êµ¬ì„±
            dataset = {
                'id': dataset_id,
                'name': dataset_name
            }
            
            for idx, doc in enumerate(documents, 1):
                doc_id = doc.get('document_id')
                file_id = doc.get('file_id')
                document_key = doc.get('document_key')
                file_name = doc.get('file_name', 'Unknown')
                
                logger.info(f"  [{idx}/{total_docs}] ì²˜ë¦¬ ì¤‘: {file_name} (ë¬¸ì„œID: {doc_id}, íŒŒì¼ID: {file_id})")
                
                deletion_success = True
                failure_reason = None
                
                # Step 1: RAGFlow knowledgebaseì—ì„œ ë¬¸ì„œ ì‚­ì œ
                if self.ragflow_client.delete_document(dataset, doc_id):
                    logger.debug(f"    âœ“ RAGFlow ë¬¸ì„œ ì‚­ì œ ì„±ê³µ")
                    ragflow_deleted += 1
                else:
                    deletion_success = False
                    failure_reason = 'RAGFlow ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨'
                    logger.warning(f"    âœ— RAGFlow ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨")
                
                # Step 2: RAGFlowì—ì„œ ì—…ë¡œë“œëœ íŒŒì¼ ì‚­ì œ (ë¬¸ì„œ ì‚­ì œ ì„±ê³µ ì‹œì—ë§Œ)
                if deletion_success and file_id:
                    if self.ragflow_client.delete_uploaded_file(file_id):
                        logger.debug(f"    âœ“ RAGFlow íŒŒì¼ ì‚­ì œ ì„±ê³µ")
                    else:
                        deletion_success = False
                        failure_reason = 'RAGFlow íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ (ë¬¸ì„œëŠ” ì‚­ì œë¨)'
                        logger.warning(f"    âœ— RAGFlow íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨")
                elif deletion_success and not file_id:
                    logger.debug(f"    âš  file_idê°€ ì—†ì–´ íŒŒì¼ ì‚­ì œ ìƒëµ")
                
                # Step 3: ëª¨ë‘ ì„±ê³µ ì‹œì—ë§Œ DBì—ì„œ ì‚­ì œ
                if deletion_success:
                    deleted_count = self.revision_db.delete_document(
                        document_key=document_key,
                        dataset_id=dataset_id,
                        file_name=file_name
                    )
                    
                    if deleted_count > 0:
                        db_deleted += deleted_count
                        logger.debug(f"    âœ“ DBì—ì„œ ì‚­ì œ ì™„ë£Œ")
                    else:
                        logger.warning(f"    âš  DB ì‚­ì œ ì‹¤íŒ¨ (RAGFlowëŠ” ì‚­ì œë¨)")
                else:
                    ragflow_failed += 1
                    failed_items.append({
                        'document_id': doc_id,
                        'file_id': file_id,
                        'file_name': file_name,
                        'reason': failure_reason
                    })
                    logger.warning(f"    âœ— ì‚­ì œ ì‹¤íŒ¨: {failure_reason} - DBëŠ” ìœ ì§€ë¨")
            
            logger.info(f"âœ“ ì‚­ì œ ì™„ë£Œ: RAGFlow {ragflow_deleted}ê°œ, DB {db_deleted}ê°œ, ì‹¤íŒ¨ {ragflow_failed}ê°œ")
            
            # ê²°ê³¼ ìš”ì•½
            logger.info("\n" + "="*80)
            logger.info("ì‚­ì œ ì‘ì—… ì™„ë£Œ")
            logger.info("-"*80)
            logger.info(f"ì§€ì‹ë² ì´ìŠ¤: {dataset_name}")
            logger.info(f"ì´ ë¬¸ì„œ ìˆ˜: {total_docs}")
            logger.info(f"RAGFlow ì‚­ì œ: {ragflow_deleted}ê°œ (ì‹¤íŒ¨: {ragflow_failed}ê°œ)")
            logger.info(f"RevisionDB ì‚­ì œ: {db_deleted}ê°œ")
            
            if failed_items:
                logger.warning(f"\nì‹¤íŒ¨í•œ ë¬¸ì„œ ëª©ë¡:")
                for item in failed_items[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                    file_id_info = f", íŒŒì¼ID: {item['file_id']}" if item.get('file_id') else ""
                    logger.warning(f"  - {item['file_name']} (ë¬¸ì„œID: {item['document_id']}{file_id_info}) - {item['reason']}")
                if len(failed_items) > 10:
                    logger.warning(f"  ... ì™¸ {len(failed_items) - 10}ê°œ")
            
            logger.info("="*80)
            
            return {
                'success': True,
                'dataset_name': dataset_name,
                'dataset_id': dataset_id,
                'total_documents': total_docs,
                'ragflow_deleted': ragflow_deleted,
                'ragflow_failed': ragflow_failed,
                'db_deleted': db_deleted,
                'failed_items': failed_items
            }
        
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'message': str(e)
            }
    
    def parse_non_failed_documents_by_dataset_name(self, dataset_name: str, monitor_progress: bool = True):
        """
        ì§€ì‹ë² ì´ìŠ¤ ë‚´ ë¬¸ì„œ ìƒíƒœë¥¼ í™•ì¸í•˜ê³ , Failed('4')ê°€ ì•„ë‹Œ ë¬¸ì„œë“¤ì„ íŒŒì‹±
        (ì´ë¯¸ ì™„ë£Œ('3')ë˜ê±°ë‚˜ ì‹¤í–‰ ì¤‘('1')ì¸ ë¬¸ì„œëŠ” ì œì™¸í•˜ê³  UNSTART('0'), CANCEL('2') ë“± ëŒ€ìƒ)
        
        Args:
            dataset_name: ì§€ì‹ë² ì´ìŠ¤ ì´ë¦„
            monitor_progress: íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì—¬ë¶€
        """
        logger.info("="*80)
        logger.info(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}' ìƒíƒœ ê¸°ë°˜ íŒŒì‹± (Non-Failed)")
        logger.info("="*80)
        
        try:
            # 1. ì§€ì‹ë² ì´ìŠ¤ ì¡°íšŒ
            dataset = self.ragflow_client.get_dataset_by_name(dataset_name)
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            logger.info(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘... (Dataset ID: {dataset.get('id')})")
            
            # 2. ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
            all_documents = []
            page = 1
            while True:
                docs = self.ragflow_client.get_documents_in_dataset(dataset, page=page, page_size=100)
                if not docs:
                    break
                all_documents.extend(docs)
                if len(docs) < 100:
                    break
                page += 1
            
            if not all_documents:
                logger.warning("ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            logger.info(f"ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ê²€ì‚¬ ì‹œì‘")

            # 3. ìƒíƒœ í•„í„°ë§
            # run status: '0': UNSTART, '1': RUNNING, '2': CANCEL, '3': DONE, '4': FAIL
            target_ids = []
            skipped_counts = {'RUNNING': 0, 'DONE': 0, 'FAIL': 0}
            
            for doc in all_documents:
                run_status = str(doc.get('run', '0'))
                doc_id = doc.get('id')
                doc_name = doc.get('name', 'Unknown')
                
                if run_status == '4':  # FAIL
                    skipped_counts['FAIL'] += 1
                    logger.debug(f"  [Skip] Failed ìƒíƒœ: {doc_name}")
                elif run_status == '3':  # DONE
                    skipped_counts['DONE'] += 1
                elif run_status == '1':  # RUNNING
                    skipped_counts['RUNNING'] += 1
                else:
                    # '0' (UNSTART), '2' (CANCEL) ë“±
                    target_ids.append(doc_id)
                    logger.debug(f"  [Target] íŒŒì‹± ëŒ€ìƒ ì¶”ê°€ (Status: {run_status}): {doc_name}")

            logger.info("-" * 40)
            logger.info(f"ìƒíƒœ ê²€ì‚¬ ê²°ê³¼:")
            logger.info(f"  - íŒŒì‹± ëŒ€ìƒ (UNSTART/CANCEL): {len(target_ids)}ê°œ")
            logger.info(f"  - ê±´ë„ˆëœ€ (ì™„ë£Œ): {skipped_counts['DONE']}ê°œ")
            logger.info(f"  - ê±´ë„ˆëœ€ (ì‹¤í–‰ì¤‘): {skipped_counts['RUNNING']}ê°œ")
            logger.info(f"  - ê±´ë„ˆëœ€ (ì‹¤íŒ¨ - ì œì™¸ë¨): {skipped_counts['FAIL']}ê°œ")
            
            if not target_ids:
                logger.info("íŒŒì‹±í•  ëŒ€ìƒ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            # 4. íŒŒì‹± ìš”ì²­
            logger.info(f"\n{len(target_ids)}ê°œ ë¬¸ì„œ íŒŒì‹± ì‹œì‘...")
            parse_started = self.ragflow_client.start_batch_parse(
                dataset,
                document_ids=target_ids
            )
            
            if parse_started and monitor_progress and MONITOR_PARSE_PROGRESS:
                self.monitor_parse_progress(dataset, dataset_name, target_ids, max_wait_minutes=PARSE_TIMEOUT_MINUTES)
            elif parse_started:
                logger.info(f"[{dataset_name}] íŒŒì‹±ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")

        except Exception as e:
            logger.error(f"ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(traceback.format_exc())

    def cancel_parsing_documents_by_dataset_name(self, dataset_name: str, confirm: bool = False):
        """
        íŠ¹ì • ë°ì´í„°ì…‹ì˜ íŒŒì‹± ì¤‘ì¸(RUNNING) ë¬¸ì„œë¥¼ íŒŒì‹± ì·¨ì†Œ
        
        Args:
            dataset_name: ì§€ì‹ë² ì´ìŠ¤ ì´ë¦„
            confirm: ì‹¤ì œ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸ í”Œë˜ê·¸
        """
        logger.info("="*80)
        logger.info(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}' íŒŒì‹± ì·¨ì†Œ (Running ìƒíƒœ ë¬¸ì„œ)")
        logger.info("="*80)
        
        try:
            # 1. ì§€ì‹ë² ì´ìŠ¤ ì¡°íšŒ
            dataset = self.ragflow_client.get_dataset_by_name(dataset_name)
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            logger.info(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘... (Dataset ID: {dataset.get('id')})")
            
            # 2. ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
            all_documents = []
            page = 1
            while True:
                docs = self.ragflow_client.get_documents_in_dataset(dataset, page=page, page_size=100)
                if not docs:
                    break
                all_documents.extend(docs)
                if len(docs) < 100:
                    break
                page += 1
            
            if not all_documents:
                logger.warning("ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            # 3. RUNNING ìƒíƒœ ë¬¸ì„œ í•„í„°ë§
            running_ids = []
            
            for doc in all_documents:
                # run status: '1': RUNNING
                run_status = str(doc.get('run', '0'))
                doc_id = doc.get('id')
                doc_name = doc.get('name', 'Unknown')
                
                if run_status == '1':  # RUNNING
                    running_ids.append(doc_id)
                    logger.debug(f"  [Running] íŒŒì‹± ì·¨ì†Œ ëŒ€ìƒ: {doc_name}")
            
            logger.info("-" * 40)
            logger.info(f"ê²€ì‚¬ ê²°ê³¼:")
            logger.info(f"  - íŒŒì‹± ì¤‘(Running) ë¬¸ì„œ: {len(running_ids)}ê°œ")
            
            if not running_ids:
                logger.info("íŒŒì‹± ì¤‘ì¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            if not confirm:
                logger.info("\nì‹¤ì œë¡œ íŒŒì‹±ì„ ì·¨ì†Œí•˜ë ¤ë©´ --confirm ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
                logger.info(f"  ì˜ˆ: python run.py --cancel-parsing \"{dataset_name}\" --confirm")
                return

            # 4. íŒŒì‹± ì·¨ì†Œ ìš”ì²­
            logger.info(f"\n{len(running_ids)}ê°œ ë¬¸ì„œ íŒŒì‹± ì·¨ì†Œ ìš”ì²­ ì¤‘...")
            if self.ragflow_client.stop_batch_parse(dataset, running_ids):
                logger.info("âœ“ íŒŒì‹± ì·¨ì†Œ ìš”ì²­ì´ ì„±ê³µì ìœ¼ë¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logger.error("âœ— íŒŒì‹± ì·¨ì†Œ ìš”ì²­ ì‹¤íŒ¨")

        except Exception as e:
            logger.error(f"ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(traceback.format_exc())

    def get_running_document_count(self, dataset) -> tuple:
        """
        íŠ¹ì • ë°ì´í„°ì…‹ì—ì„œ í˜„ì¬ íŒŒì‹± ì¤‘(RUNNING)ì¸ ë¬¸ì„œ ìˆ˜ì™€ ì „ì²´ ìƒíƒœ ì •ë³´ë¥¼ ë°˜í™˜
        
        Args:
            dataset: ë°ì´í„°ì…‹ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            (running_count, status_counts) íŠœí”Œ
            - running_count: RUNNING ìƒíƒœ ë¬¸ì„œ ìˆ˜
            - status_counts: ìƒíƒœë³„ ë¬¸ì„œ ìˆ˜ ë”•ì…”ë„ˆë¦¬
        """
        try:
            all_documents = []
            page = 1
            while True:
                docs = self.ragflow_client.get_documents_in_dataset(dataset, page=page, page_size=100)
                if not docs:
                    break
                all_documents.extend(docs)
                if len(docs) < 100:
                    break
                page += 1
            
            status_counts = {'UNSTART': 0, 'RUNNING': 0, 'CANCEL': 0, 'DONE': 0, 'FAIL': 0, 'TOTAL': len(all_documents)}
            
            for doc in all_documents:
                run_status = str(doc.get('run', '0'))
                if run_status == '0':
                    status_counts['UNSTART'] += 1
                elif run_status == '1':
                    status_counts['RUNNING'] += 1
                elif run_status == '2':
                    status_counts['CANCEL'] += 1
                elif run_status == '3':
                    status_counts['DONE'] += 1
                elif run_status == '4':
                    status_counts['FAIL'] += 1
            
            return status_counts['RUNNING'], status_counts
            
        except Exception as e:
            logger.error(f"RUNNING ë¬¸ì„œ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0, {}

    def throttle_parse_by_dataset_name(
        self,
        dataset_name: str,
        confirm: bool = False,
        concurrency_limit: int = None,
        include_done: bool = False,
        include_failed: bool = False,
        check_interval: int = 10,
        max_hours: float = 2.0
    ):
        """
        ë™ì‹œ íŒŒì‹± ìˆ˜ë¥¼ ì œí•œí•˜ë©´ì„œ ì „ì²´ ë¬¸ì„œ íŒŒì‹± ìˆ˜í–‰
        
        í˜„ì¬ RUNNING ìƒíƒœì˜ ë¬¸ì„œ ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë™ì‹œ íŒŒì‹± ìˆ˜ë¥¼ ì œí•œí•˜ì—¬
        ì„œë²„ ë¶€í•˜ë¥¼ ì¡°ì ˆí•˜ë©´ì„œ ì „ì²´ ë¬¸ì„œë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            dataset_name: ì§€ì‹ë² ì´ìŠ¤ ì´ë¦„ (íŠ¹ìˆ˜ê°’ "ALL"ì´ë©´ ëª¨ë“  ë°ì´í„°ì…‹)
            confirm: Trueì—¬ì•¼ ì‹¤ì œ ì‹¤í–‰
            concurrency_limit: ë™ì‹œ íŒŒì‹± ìˆ˜ ì œí•œ (Noneì´ë©´ í˜„ì¬ RUNNING ìˆ˜ ì‚¬ìš©)
            include_done: DONE ìƒíƒœ ë¬¸ì„œ í¬í•¨ ì—¬ë¶€ (ì¬íŒŒì‹±)
            include_failed: FAIL ìƒíƒœ ë¬¸ì„œ í¬í•¨ ì—¬ë¶€
            check_interval: ìƒíƒœ í™•ì¸ ê°„ê²© (ì´ˆ)
            max_hours: ìµœëŒ€ ë™ì‘ ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸: 2ì‹œê°„)
        """
        # "ALL" ì˜µì…˜ ì²˜ë¦¬ - ëª¨ë“  ë°ì´í„°ì…‹ ëŒ€ìƒ
        if dataset_name.upper() == "ALL":
            logger.info("=" * 80)
            logger.info("ëª¨ë“  ì§€ì‹ë² ì´ìŠ¤ ë™ì‹œì„± ì œí•œ íŒŒì‹± (Throttled Parse ALL)")
            logger.info("=" * 80)
            
            # ì „ì²´ ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ
            all_datasets = self.ragflow_client.list_datasets(page=1, page_size=1000)
            if not all_datasets:
                logger.warning("ì§€ì‹ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            logger.info(f"ì´ {len(all_datasets)}ê°œ ì§€ì‹ë² ì´ìŠ¤ ë°œê²¬:")
            for ds in all_datasets:
                logger.info(f"  - {ds.get('name')} (ID: {ds.get('id')})")
            
            if not confirm:
                logger.info("\n" + "=" * 80)
                logger.info("ì‹¤ì œë¡œ íŒŒì‹±ì„ ì‹¤í–‰í•˜ë ¤ë©´ --confirm ì˜µì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”.")
                logger.info('  ì˜ˆ: python run.py --throttle-parse "ALL" --confirm')
                logger.info(f"\nì˜ˆìƒ ë™ì‘:")
                logger.info(f"  - ëŒ€ìƒ: ëª¨ë“  ì§€ì‹ë² ì´ìŠ¤ ({len(all_datasets)}ê°œ)")
                logger.info(f"  - ë™ì‹œ íŒŒì‹± ìˆ˜: {concurrency_limit or 'ìë™ ê°ì§€'}ê°œ")
                logger.info(f"  - í™•ì¸ ê°„ê²©: {check_interval}ì´ˆ")
                logger.info(f"  - ìµœëŒ€ ë™ì‘ ì‹œê°„: {max_hours}ì‹œê°„")
                logger.info("=" * 80)
                return
            
            # ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ íŒŒì‹± ì‹¤í–‰
            logger.info("\n" + "=" * 80)
            logger.info("ëª¨ë“  ì§€ì‹ë² ì´ìŠ¤ íŒŒì‹± ì‹œì‘")
            logger.info("=" * 80)
            
            for idx, dataset in enumerate(all_datasets, 1):
                ds_name = dataset.get('name')
                logger.info(f"\n[{idx}/{len(all_datasets)}] ì²˜ë¦¬ ì¤‘: {ds_name}")
                logger.info("-" * 60)
                
                try:
                    # ê°œë³„ ë°ì´í„°ì…‹ íŒŒì‹± (ì¬ê·€ í˜¸ì¶œ)
                    self.throttle_parse_by_dataset_name(
                        dataset_name=ds_name,
                        confirm=True,  # ì´ë¯¸ í™•ì¸í–ˆìœ¼ë¯€ë¡œ True
                        concurrency_limit=concurrency_limit,
                        include_done=include_done,
                        include_failed=include_failed,
                        check_interval=check_interval,
                        max_hours=max_hours
                    )
                except Exception as e:
                    logger.error(f"ì§€ì‹ë² ì´ìŠ¤ '{ds_name}' íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
                    logger.info("ë‹¤ìŒ ì§€ì‹ë² ì´ìŠ¤ë¡œ ê³„ì†...")
            
            logger.info("\n" + "=" * 80)
            logger.info("ëª¨ë“  ì§€ì‹ë² ì´ìŠ¤ íŒŒì‹± ì™„ë£Œ")
            logger.info("=" * 80)
            return
        
        # ê¸°ì¡´ ë¡œì§: íŠ¹ì • ë°ì´í„°ì…‹ íŒŒì‹±
        logger.info("=" * 80)
        logger.info(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}' ë™ì‹œì„± ì œí•œ íŒŒì‹± (Throttled Parse)")
        logger.info("=" * 80)
        
        try:
            # 1. ì§€ì‹ë² ì´ìŠ¤ ì¡°íšŒ
            dataset = self.ragflow_client.get_dataset_by_name(dataset_name)
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            dataset_id = dataset.get('id')
            logger.info(f"Dataset ID: {dataset_id}")
            
            # 2. í˜„ì¬ ìƒíƒœ ì¡°íšŒ
            running_count, status_counts = self.get_running_document_count(dataset)
            
            logger.info("-" * 40)
            logger.info("í˜„ì¬ ë¬¸ì„œ ìƒíƒœ:")
            logger.info(f"  - ì´ ë¬¸ì„œ: {status_counts.get('TOTAL', 0)}ê°œ")
            logger.info(f"  - UNSTART: {status_counts.get('UNSTART', 0)}ê°œ")
            logger.info(f"  - RUNNING: {status_counts.get('RUNNING', 0)}ê°œ")
            logger.info(f"  - CANCEL: {status_counts.get('CANCEL', 0)}ê°œ")
            logger.info(f"  - DONE: {status_counts.get('DONE', 0)}ê°œ")
            logger.info(f"  - FAIL: {status_counts.get('FAIL', 0)}ê°œ")
            
            # 3. ë™ì‹œì„± ì œí•œ ì„¤ì •
            if concurrency_limit is None:
                if running_count > 0:
                    concurrency_limit = running_count
                    logger.info(f"\nâœ“ ë™ì‹œì„± ì œí•œ: í˜„ì¬ RUNNING ìˆ˜ ê¸°ì¤€ â†’ {concurrency_limit}ê°œ")
                else:
                    concurrency_limit = 5  # ê¸°ë³¸ê°’
                    logger.info(f"\nâš  í˜„ì¬ RUNNING ë¬¸ì„œê°€ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©: {concurrency_limit}ê°œ")
            else:
                logger.info(f"\nâœ“ ë™ì‹œì„± ì œí•œ: ì‚¬ìš©ì ì§€ì • â†’ {concurrency_limit}ê°œ")
            
            # 4. íŒŒì‹± ëŒ€ìƒ ë¬¸ì„œ ìˆ˜ì§‘
            all_documents = []
            page = 1
            while True:
                docs = self.ragflow_client.get_documents_in_dataset(dataset, page=page, page_size=100)
                if not docs:
                    break
                all_documents.extend(docs)
                if len(docs) < 100:
                    break
                page += 1
            
            # íŒŒì‹± ëŒ€ìƒ: UNSTART, CANCEL, (ì˜µì…˜) DONE, (ì˜µì…˜) FAIL
            pending_ids = []
            for doc in all_documents:
                run_status = str(doc.get('run', '0'))
                doc_id = doc.get('id')
                
                if run_status in ['0', '2']:  # UNSTART, CANCEL
                    pending_ids.append(doc_id)
                elif run_status == '3' and include_done:  # DONE (ì¬íŒŒì‹±)
                    pending_ids.append(doc_id)
                elif run_status == '4' and include_failed:  # FAIL
                    pending_ids.append(doc_id)
            
            total_pending = len(pending_ids)
            logger.info(f"íŒŒì‹± ëŒ€ê¸° ë¬¸ì„œ: {total_pending}ê°œ")
            logger.info(f"  - ì˜µì…˜: include_done={include_done}, include_failed={include_failed}")
            
            if total_pending == 0:
                logger.info("íŒŒì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            if not confirm:
                logger.info("\n" + "-" * 40)
                logger.info("ì‹¤ì œë¡œ íŒŒì‹±ì„ ì‹¤í–‰í•˜ë ¤ë©´ --confirm ì˜µì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”.")
                logger.info(f'  ì˜ˆ: python run.py --throttle-parse "{dataset_name}" --confirm')
                logger.info(f"\nì˜ˆìƒ ë™ì‘:")
                logger.info(f"  - ë™ì‹œ íŒŒì‹± ìˆ˜: {concurrency_limit}ê°œ ìœ ì§€")
                logger.info(f"  - íŒŒì‹± ëŒ€ìƒ: {total_pending}ê°œ")
                logger.info(f"  - í™•ì¸ ê°„ê²©: {check_interval}ì´ˆ")
                logger.info(f"  - ìµœëŒ€ ë™ì‘ ì‹œê°„: {max_hours}ì‹œê°„")
                return
            
            # 5. ë™ì‹œì„± ì œí•œ íŒŒì‹± ì‹¤í–‰
            logger.info("\n" + "=" * 80)
            logger.info("ë™ì‹œì„± ì œí•œ íŒŒì‹± ì‹œì‘")
            logger.info(f"ìµœëŒ€ ë™ì‘ ì‹œê°„: {max_hours}ì‹œê°„")
            logger.info("=" * 80)
            
            start_time = time.time()
            max_wait_seconds = max_hours * 3600  # ì‹œê°„ -> ì´ˆ
            
            submitted_ids = set()  # ì´ë¯¸ íŒŒì‹± ìš”ì²­í•œ ë¬¸ì„œ
            completed_ids = set()  # ì™„ë£Œëœ ë¬¸ì„œ
            
            # ì „ì²´ ë¬¸ì„œ ëª©ë¡ì€ ì´ë¯¸ ìœ„ì—ì„œ ì¡°íšŒí–ˆìœ¼ë¯€ë¡œ all_documents ì‚¬ìš©
            # pending_idsë„ ì´ë¯¸ ê³„ì‚°ë¨
            logger.info(f"âœ“ ì „ì²´ ë¬¸ì„œ ëª©ë¡ ìºì‹œ ì™„ë£Œ: {len(all_documents)}ê°œ (ë§¤ ë°˜ë³µë§ˆë‹¤ ì¬ì¡°íšŒí•˜ì§€ ì•ŠìŒ)")
            
            while len(completed_ids) < total_pending:
                # ì œì¶œí–ˆì§€ë§Œ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì€ ë¬¸ì„œ IDë“¤
                in_progress_ids = list(submitted_ids - completed_ids)
                
                # ì œì¶œí•œ ë¬¸ì„œë“¤ì˜ í˜„ì¬ ìƒíƒœë§Œ ê°œë³„ ì¡°íšŒ (ì „ì²´ ëª©ë¡ ì¡°íšŒ ëŒ€ì‹ )
                our_running = 0
                if in_progress_ids:
                    # ì§„í–‰ ì¤‘ì¸ ë¬¸ì„œë“¤ì˜ ìƒíƒœë§Œ ì¡°íšŒ
                    in_progress_docs = self.ragflow_client.get_documents_by_ids(dataset, in_progress_ids)
                    
                    for doc in in_progress_docs:
                        doc_id = doc.get('id')
                        run_status = str(doc.get('run', '0'))
                        
                        if run_status in ['3', '4']:  # DONE or FAIL
                            completed_ids.add(doc_id)
                        elif run_status == '1':  # RUNNING
                            our_running += 1
                
                # ì¶”ê°€ ê°€ëŠ¥í•œ ìŠ¬ë¡¯ ê³„ì‚°
                available_slots = concurrency_limit - our_running
                
                # ì•„ì§ ì œì¶œí•˜ì§€ ì•Šì€ ë¬¸ì„œ ì¤‘ ì¶”ê°€
                if available_slots > 0:
                    to_submit = []
                    for doc_id in pending_ids:
                        if doc_id not in submitted_ids and len(to_submit) < available_slots:
                            to_submit.append(doc_id)
                    
                    if to_submit:
                        logger.info(f"[{len(completed_ids)}/{total_pending}] "
                                   f"RUNNING: {our_running} â†’ ì¶”ê°€ ìš”ì²­: {len(to_submit)}ê°œ")
                        
                        # íŒŒì‹± ìš”ì²­
                        if self.ragflow_client.start_batch_parse(dataset, document_ids=to_submit):
                            for doc_id in to_submit:
                                submitted_ids.add(doc_id)
                        else:
                            logger.warning("íŒŒì‹± ìš”ì²­ ì‹¤íŒ¨, ì¬ì‹œë„ ì˜ˆì •...")
                else:
                    # ì§„í–‰ ìƒí™© ë¡œê·¸
                    if len(completed_ids) % 10 == 0 or our_running > 0:
                        logger.info(f"[{len(completed_ids)}/{total_pending}] "
                                   f"RUNNING: {our_running}/{concurrency_limit}")
                
                # íƒ€ì„ì•„ì›ƒ ì²´í¬
                elapsed = time.time() - start_time
                if elapsed > max_wait_seconds:
                    logger.warning(f"â±ï¸ ìµœëŒ€ ë™ì‘ ì‹œê°„ ì´ˆê³¼ ({max_hours}ì‹œê°„)")
                    logger.info(f"ì§„í–‰ ìƒí™©: {len(completed_ids)}/{total_pending} ì™„ë£Œ")
                    break
                
                # ëª¨ë“  ë¬¸ì„œê°€ ì œì¶œë˜ê³  ì™„ë£Œë˜ë©´ ì¢…ë£Œ
                if len(submitted_ids) >= total_pending and len(completed_ids) >= len(submitted_ids):
                    break
                
                # ëŒ€ê¸°
                time.sleep(check_interval)
            
            # ìµœì¢… ìƒíƒœ í™•ì¸
            _, final_status = self.get_running_document_count(dataset)
            
            elapsed_time = time.time() - start_time
            elapsed_minutes = elapsed_time / 60
            
            logger.info("\n" + "=" * 80)
            logger.info("íŒŒì‹± ì™„ë£Œ")
            logger.info("-" * 40)
            logger.info(f"ì†Œìš” ì‹œê°„: {elapsed_minutes:.1f}ë¶„")
            logger.info(f"ì™„ë£Œëœ ë¬¸ì„œ: {len(completed_ids)}/{total_pending}ê°œ")
            logger.info(f"ìµœì¢… ìƒíƒœ:")
            logger.info(f"  - DONE: {final_status.get('DONE', 0)}ê°œ")
            logger.info(f"  - FAIL: {final_status.get('FAIL', 0)}ê°œ")
            logger.info(f"  - RUNNING: {final_status.get('RUNNING', 0)}ê°œ")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"ë™ì‹œì„± ì œí•œ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(traceback.format_exc())

    def reparse_all_documents_by_dataset_name(
        self,
        dataset_name: str,
        confirm: bool = False,
        cancel_running: bool = False,
        include_running: bool = False,
        include_failed: bool = True,
        monitor_progress: bool = True,
        page_size: int = 100
    ):
        """
        íŠ¹ì • ì§€ì‹ë² ì´ìŠ¤ì˜ "ëª¨ë“  ë¬¸ì„œ"ë¥¼ ì¬íŒŒì‹±í•©ë‹ˆë‹¤.
        
        ê¸°ë³¸ ë™ì‘:
        - ë¬¸ì„œ ëª©ë¡ì„ ì „ë¶€ ì¡°íšŒ
        - RUNNING ë¬¸ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì œì™¸ (include_running=False)
        - confirm=Trueì¼ ë•Œë§Œ ì¬íŒŒì‹±ì„ ì‹œì‘ (ì„œë²„ê°€ ê¸°ì¡´ ì²­í¬/ì‘ì—…ì„ ì •ë¦¬í•˜ê³  ì¬íì‰í•¨)
        
        Args:
            dataset_name: ì§€ì‹ë² ì´ìŠ¤ ì´ë¦„
            confirm: Trueì—¬ì•¼ ì‹¤ì œ ì‹¤í–‰ (ì¬íŒŒì‹± ì‹œ ì„œë²„ê°€ ê¸°ì¡´ ì²­í¬/ì‘ì—…ì„ ì •ë¦¬í•˜ë¯€ë¡œ ì•ˆì „ì¥ì¹˜)
            cancel_running: (í˜¸í™˜ ìœ ì§€) Trueë©´ RUNNING ë¬¸ì„œë„ ëŒ€ìƒìœ¼ë¡œ í¬í•¨í•˜ë„ë¡ í—ˆìš©
            include_running: Trueë©´ RUNNING ë¬¸ì„œë„ "ì¬íŒŒì‹± ëŒ€ìƒ"ìœ¼ë¡œ í¬í•¨ (cancel_runningê³¼ í•¨ê»˜ ì“°ëŠ” ê²ƒì„ ê¶Œì¥)
            include_failed: Trueë©´ FAIL ë¬¸ì„œë„ í¬í•¨
            monitor_progress: ì§„í–‰ ëª¨ë‹ˆí„°ë§ ì—¬ë¶€
            page_size: ë¬¸ì„œ ëª©ë¡ í˜ì´ì§€ í¬ê¸°
        """
        logger.info("=" * 80)
        logger.info(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}' ì „ì²´ ë¬¸ì„œ ì¬íŒŒì‹±")
        logger.info("=" * 80)
        
        try:
            dataset = self.ragflow_client.get_dataset_by_name(dataset_name)
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 1) ë¬¸ì„œ ëª©ë¡ ì „ì²´ ìˆ˜ì§‘
            all_documents = []
            page = 1
            while True:
                docs = self.ragflow_client.get_documents_in_dataset(dataset, page=page, page_size=page_size)
                if not docs:
                    break
                all_documents.extend(docs)
                if len(docs) < page_size:
                    break
                page += 1
            
            if not all_documents:
                logger.warning("ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 2) ëŒ€ìƒ ë¬¸ì„œ ì„ ì •
            # run status: '0': UNSTART, '1': RUNNING, '2': CANCEL, '3': DONE, '4': FAIL
            target_ids = []
            doc_run_map = {}
            counts = {'TOTAL': len(all_documents), 'RUNNING': 0, 'DONE': 0, 'UNSTART': 0, 'CANCEL': 0, 'FAIL': 0, 'UNKNOWN': 0}
            
            for doc in all_documents:
                run_status = str(doc.get('run', '0'))
                doc_id = doc.get('id')
                if not doc_id:
                    continue
                doc_run_map[doc_id] = run_status
                
                if run_status == '1':
                    counts['RUNNING'] += 1
                    if include_running:
                        target_ids.append(doc_id)
                elif run_status == '3':
                    counts['DONE'] += 1
                    target_ids.append(doc_id)
                elif run_status == '4':
                    counts['FAIL'] += 1
                    if include_failed:
                        target_ids.append(doc_id)
                elif run_status == '2':
                    counts['CANCEL'] += 1
                    target_ids.append(doc_id)
                elif run_status == '0':
                    counts['UNSTART'] += 1
                    target_ids.append(doc_id)
                else:
                    counts['UNKNOWN'] += 1
                    target_ids.append(doc_id)
            
            # ì¤‘ë³µ ì œê±° (ë°©ì–´)
            target_ids = list(dict.fromkeys(target_ids))
            
            logger.info("-" * 40)
            logger.info("ë¬¸ì„œ ìƒíƒœ ìš”ì•½:")
            logger.info(f"  - ì´ ë¬¸ì„œ: {counts['TOTAL']}ê°œ")
            logger.info(f"  - UNSTART: {counts['UNSTART']}ê°œ")
            logger.info(f"  - CANCEL: {counts['CANCEL']}ê°œ")
            logger.info(f"  - DONE: {counts['DONE']}ê°œ")
            logger.info(f"  - FAIL: {counts['FAIL']}ê°œ (include_failed={include_failed})")
            logger.info(f"  - RUNNING: {counts['RUNNING']}ê°œ (include_running={include_running})")
            if counts['UNKNOWN'] > 0:
                logger.info(f"  - UNKNOWN: {counts['UNKNOWN']}ê°œ")
            logger.info(f"ì¬íŒŒì‹± ëŒ€ìƒ ë¬¸ì„œ: {len(target_ids)}ê°œ")
            
            if not target_ids:
                logger.info("ì¬íŒŒì‹±í•  ëŒ€ìƒ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            if not confirm:
                logger.info("\nì‹¤ì œë¡œ ì¬íŒŒì‹±ì„ ì‹¤í–‰í•˜ë ¤ë©´ --confirm ì˜µì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”.")
                logger.info(f'  ì˜ˆ: python run.py --reparse-all "{dataset_name}" --confirm')
                logger.info("ì£¼ì˜: --confirm ì‹¤í–‰ ì‹œ ëŒ€ìƒ ë¬¸ì„œì˜ ê¸°ì¡´ ì²­í¬ê°€ ì‚­ì œ(ë¦¬ì…‹)ë©ë‹ˆë‹¤.")
                return
            
            # 3) RUNNING ë¬¸ì„œ ì²˜ë¦¬ ì •ì±…
            if counts['RUNNING'] > 0 and include_running and not cancel_running:
                logger.warning("RUNNING ë¬¸ì„œë¥¼ í¬í•¨í•˜ë ¤ë©´ --cancel-running ì˜µì…˜ì„ í•¨ê»˜ ì‚¬ìš©í•˜ì„¸ìš”. (ì•ˆì „)")
                logger.warning("í˜„ì¬ ì„¤ì •ì—ì„œëŠ” RUNNING ë¬¸ì„œë¥¼ ëŒ€ìƒì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.")
                # ëŒ€ìƒì—ì„œ RUNNING ë¬¸ì„œë¥¼ ì œê±°
                running_ids = {d.get('id') for d in all_documents if str(d.get('run', '0')) == '1'}
                target_ids = [i for i in target_ids if i not in running_ids]
            
            # 4) ì¬íŒŒì‹± ì‹œì‘
            # ì°¸ê³ : RAGFlow v21 API(POST /datasets/{id}/chunks)ëŠ” ë‚´ë¶€ì—ì„œ ê¸°ì¡´ chunk/index/taskë¥¼ ì •ë¦¬í•˜ê³  ì¬íì‰í•œë‹¤.
            logger.info("\nì¬íŒŒì‹± ì‹œì‘...")
            parse_started = self.ragflow_client.start_batch_parse(dataset, document_ids=target_ids)
            if not parse_started:
                logger.error("ì¬íŒŒì‹± ìš”ì²­ ì‹¤íŒ¨")
                return
            
            if monitor_progress and MONITOR_PARSE_PROGRESS:
                self.monitor_parse_progress(dataset, dataset_name, target_ids, max_wait_minutes=PARSE_TIMEOUT_MINUTES)
            else:
                logger.info(f"[{dataset_name}] ì¬íŒŒì‹±ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
        
        except Exception as e:
            logger.error(f"ì „ì²´ ì¬íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(traceback.format_exc())

    def print_statistics(self):
        """ì²˜ë¦¬ í†µê³„ ì¶œë ¥"""
        logger.info("="*80)
        logger.info("ë°°ì¹˜ ì²˜ë¦¬ í†µê³„")
        logger.info("-"*80)
        
        # ì‹œíŠ¸ í†µê³„
        logger.info(f"ì´ ì‹œíŠ¸ ìˆ˜: {self.stats['total_sheets']}")
        logger.info(f"  - ê±´ë„ˆë›´ ì‹œíŠ¸ (ëª©ì°¨): {self.stats['skipped_sheets']}")
        logger.info(f"  - Revision ê´€ë¦¬ ì‹œíŠ¸: {self.stats['revision_sheets']}")
        logger.info(f"  - ì²¨ë¶€íŒŒì¼ ì‹œíŠ¸: {self.stats['attachment_sheets']}")
        logger.info(f"  - ì´ë ¥ê´€ë¦¬/ì†Œí”„íŠ¸ì›¨ì–´ ì‹œíŠ¸: {self.stats['history_sheets']}")
        logger.info(f"ìƒì„±ëœ ì§€ì‹ë² ì´ìŠ¤ ìˆ˜: {self.stats['datasets_created']}")
        
        logger.info("-"*80)
        
        # Revision ê´€ë¦¬ í†µê³„
        if self.stats['revision_sheets'] > 0:
            logger.info(f"Revision ê´€ë¦¬ ë¬¸ì„œ:")
            logger.info(f"  - ì‹ ê·œ ë¬¸ì„œ: {self.stats['new_documents']}")
            logger.info(f"  - ì—…ë°ì´íŠ¸ ë¬¸ì„œ: {self.stats['updated_documents']}")
            logger.info(f"  - ê±´ë„ˆë›´ ë¬¸ì„œ (ë™ì¼ revision): {self.stats['skipped_documents']}")
            logger.info(f"  - ì‚­ì œëœ ë¬¸ì„œ: {self.stats['deleted_documents']}")
            if self.stats['failed_deletions'] > 0:
                logger.info(f"  - ì‚­ì œ ì‹¤íŒ¨: {self.stats['failed_deletions']}")
            logger.info("-"*80)
        
        # íŒŒì¼ ì—…ë¡œë“œ í†µê³„
        logger.info(f"ì´ íŒŒì¼ ìˆ˜: {self.stats['total_files']}")
        logger.info(f"ì—…ë¡œë“œ ì„±ê³µ: {self.stats['successful_uploads']}")
        logger.info(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {self.stats['failed_uploads']}")
        
        if self.stats['total_files'] > 0:
            success_rate = (self.stats['successful_uploads'] / self.stats['total_files']) * 100
            logger.info(f"ì—…ë¡œë“œ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        logger.info("-"*80)
        
        # ë‹¤ìš´ë¡œë“œ ìºì‹œ í†µê³„
        try:
            db_stats = self.revision_db.get_statistics()
            cached_downloads = db_stats.get('cached_downloads', 0)
            if cached_downloads > 0:
                logger.info(f"ë‹¤ìš´ë¡œë“œ ìºì‹œ: {cached_downloads}ê°œ URL ìºì‹œë¨")
                logger.info("-"*80)
        except Exception as e:
            logger.debug(f"ë‹¤ìš´ë¡œë“œ ìºì‹œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        logger.info("="*80)

    def sync_dataset_with_db(self, dataset_name: str, fix: bool = False) -> Dict:
        """
        RAGFlow Datasetê³¼ RevisionDB ê°„ì˜ ë°ì´í„° ì •í•©ì„± ê²€ì‚¬ ë° ë™ê¸°í™”
        
        Args:
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
            fix: Trueì´ë©´ ë¶ˆì¼ì¹˜ í•­ëª© ìë™ ìˆ˜ì • (RAGFlowì—ì„œ ê³ ì•„ ë¬¸ì„œ ì‚­ì œ)
            
        Returns:
            ê²€ì‚¬ ê²°ê³¼ (orphans, ghosts ë“±)
        """
        logger.info("="*80)
        logger.info(f"ë°ì´í„° ì •í•©ì„± ê²€ì‚¬ (Sync Check): {dataset_name}")
        logger.info("="*80)
        
        result = {
            'success': False,
            'ragflow_count': 0,
            'db_count': 0,
            'orphans': [],  # RAGFlowì—ë§Œ ìˆìŒ (ì‚­ì œ ëŒ€ìƒ)
            'ghosts': [],   # DBì—ë§Œ ìˆìŒ (DBì—ì„œ ì‚­ì œ ëŒ€ìƒ)
            'fixed_count': 0
        }
        
        try:
            # 1. RAGFlow ë°ì´í„° ì¡°íšŒ
            dataset = self.ragflow_client.get_dataset_by_name(dataset_name)
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return result
                
            dataset_id = dataset.get('id')
            logger.info(f"RAGFlow ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘... (Dataset ID: {dataset_id})")
            
            ragflow_docs = []
            page = 1
            while True:
                docs = self.ragflow_client.get_documents_in_dataset(dataset, page=page, page_size=100)
                if not docs:
                    break
                ragflow_docs.extend(docs)
                if len(docs) < 100:
                    break
                page += 1
            
            result['ragflow_count'] = len(ragflow_docs)
            ragflow_map = {d['id']: d for d in ragflow_docs}
            logger.info(f"âœ“ RAGFlow ë¬¸ì„œ: {len(ragflow_docs)}ê°œ")
            
            # 2. RevisionDB ë°ì´í„° ì¡°íšŒ
            logger.info("RevisionDB ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘...")
            db_docs = self.revision_db.get_documents_by_dataset_name(dataset_name)
            result['db_count'] = len(db_docs)
            db_map = {d['document_id']: d for d in db_docs}
            logger.info(f"âœ“ RevisionDB ë¬¸ì„œ: {len(db_docs)}ê°œ")
            
            # 3. ë¶ˆì¼ì¹˜ ë¶„ì„
            # Orphans: RAGFlowì—ëŠ” ìˆëŠ”ë° DBì—ëŠ” ì—†ëŠ” ê²ƒ (ì‚­ì œí•´ì•¼ í•¨)
            for doc_id, doc in ragflow_map.items():
                if doc_id not in db_map:
                    result['orphans'].append({
                        'id': doc_id,
                        'name': doc.get('name')
                    })
            
            # Ghosts: DBì—ëŠ” ìˆëŠ”ë° RAGFlowì—ëŠ” ì—†ëŠ” ê²ƒ (DBì—ì„œ ì‚­ì œí•´ì•¼ í•¨)
            for doc_id, doc in db_map.items():
                if doc_id not in ragflow_map:
                    result['ghosts'].append({
                        'id': doc_id,
                        'key': doc.get('document_key'),
                        'name': doc.get('file_name')
                    })
            
            logger.info("-" * 40)
            logger.info(f"ë¶„ì„ ê²°ê³¼:")
            logger.info(f"  - ì •ìƒ ë§¤ì¹­: {len(ragflow_docs) - len(result['orphans'])}ê°œ")
            logger.info(f"  - ê³ ì•„ ë¬¸ì„œ (RAGFlow Only): {len(result['orphans'])}ê°œ {'(ì‚­ì œ í•„ìš”)' if result['orphans'] else ''}")
            logger.info(f"  - ìœ ë ¹ ë¬¸ì„œ (DB Only): {len(result['ghosts'])}ê°œ {'(DB ì •ë¦¬ í•„ìš”)' if result['ghosts'] else ''}")
            
            # 4. ìˆ˜ì • (Fix)
            if fix and (result['orphans'] or result['ghosts']):
                logger.info("-" * 40)
                logger.info("ìë™ ë³µêµ¬(Fix) ì‹œì‘...")
                
                # ê³ ì•„ ë¬¸ì„œ ì‚­ì œ (RAGFlowì—ì„œ ì‚­ì œ)
                for item in result['orphans']:
                    doc_id = item['id']
                    doc_name = item['name']
                    if self.ragflow_client.delete_document(dataset, doc_id):
                        logger.info(f"  âœ“ ê³ ì•„ ë¬¸ì„œ ì‚­ì œë¨: {doc_name} ({doc_id})")
                        result['fixed_count'] += 1
                    else:
                        logger.error(f"  âœ— ê³ ì•„ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {doc_name}")
                
                # ìœ ë ¹ ë¬¸ì„œ ì‚­ì œ (DBì—ì„œ ì‚­ì œ)
                for item in result['ghosts']:
                    doc_id = item['id']
                    doc_key = item['key']
                    # ìœ ë ¹ ë¬¸ì„œëŠ” ì´ë¯¸ RAGFlowì— ì—†ìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì‚­ì œ ê°€ëŠ¥
                    self.revision_db.delete_document(doc_key, dataset_id, item['name'])
                    logger.info(f"  âœ“ DB ìœ ë ¹ ë ˆì½”ë“œ ì‚­ì œë¨: {item['name']} ({doc_key})")
                    result['fixed_count'] += 1
                
                logger.info("ë³µêµ¬ ì™„ë£Œ")
            
            result['success'] = True
            return result
            
        except Exception as e:
            logger.error(f"ë™ê¸°í™” ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return result


