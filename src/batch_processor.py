"""
ë°°ì¹˜ í”„ë¡œì„¸ì„œ - ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¡°ìœ¨
"""
import time
from pathlib import Path
from typing import Dict, List, Optional
from excel_processor import ExcelProcessor, SheetType
from file_handler import FileHandler
from ragflow_client import RAGFlowClient  # HTTP API í´ë¼ì´ì–¸íŠ¸
from revision_db import RevisionDB  # Revision ê´€ë¦¬ DB
from logger import logger
from config import (
    EXCEL_FILE_PATH, 
    DATASET_PERMISSION, 
    EMBEDDING_MODEL,
    DATA_SOURCE,
    DB_CONNECTION_STRING,
    CHUNK_METHOD,
    PARSER_CONFIG,
    MONITOR_PARSE_PROGRESS,
    PARSE_TIMEOUT_MINUTES,
    ENABLE_REVISION_MANAGEMENT,
    SKIP_SAME_REVISION,
    DELETE_BEFORE_UPLOAD,
    HISTORY_SHEET_UPLOAD_FORMAT,
    TEMP_DIR
)


class BatchProcessor:
    """ë°°ì¹˜ ì²˜ë¦¬ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, excel_path: str = None, data_source: str = None):
        """
        Args:
            excel_path: ì—‘ì…€ íŒŒì¼ ê²½ë¡œ
            data_source: ë°ì´í„° ì†ŒìŠ¤ ("excel", "db", "both")
        """
        self.excel_path = excel_path or EXCEL_FILE_PATH
        self.data_source = data_source or DATA_SOURCE
        
        # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.excel_processor = None
        self.db_processor = None
        
        # Excel ì†ŒìŠ¤
        if self.data_source in ['excel', 'both']:
            self.excel_processor = ExcelProcessor(self.excel_path)
        
        # DB ì†ŒìŠ¤
        if self.data_source in ['db', 'both']:
            self._init_db_processor()
        
        self.file_handler = FileHandler()
        self.ragflow_client = RAGFlowClient()
        self.revision_db = RevisionDB()  # Revision ê´€ë¦¬ DB
        
        self.stats = {
            'total_sheets': 0,
            'skipped_sheets': 0,  # ëª©ì°¨ ë“±
            'revision_sheets': 0,  # REV/ì‘ì„±ë²„ì „ ê´€ë¦¬
            'attachment_sheets': 0,  # ì²¨ë¶€íŒŒì¼
            'history_sheets': 0,  # ì´ë ¥ê´€ë¦¬+ì†Œí”„íŠ¸ì›¨ì–´
            
            'new_documents': 0,  # ì‹ ê·œ ë¬¸ì„œ
            'updated_documents': 0,  # ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œ
            'skipped_documents': 0,  # ë™ì¼ revision
            'deleted_documents': 0,  # ì‚­ì œëœ ë¬¸ì„œ
            'failed_deletions': 0,  # ì‚­ì œ ì‹¤íŒ¨
            
            'total_files': 0,
            'successful_uploads': 0,
            'failed_uploads': 0,
            'datasets_created': 0
        }
    
    def _init_db_processor(self):
        """DB í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”"""
        try:
            from db_connector import DBConnector
            from db_processor import DBProcessor
            
            # DB ì—°ê²° ë¬¸ìì—´ í™•ì¸
            if not DB_CONNECTION_STRING:
                logger.warning("DB ì—°ê²° ë¬¸ìì—´ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. DB ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                self.data_source = 'excel'  # ê°•ì œë¡œ Excelë§Œ ì²˜ë¦¬
                return
            
            connector = DBConnector(connection_string=DB_CONNECTION_STRING)
            # FileHandlerë¥¼ ì „ë‹¬í•˜ì—¬ DB ë°ì´í„°ë¥¼ PDFë¡œ ë³€í™˜
            self.db_processor = DBProcessor(connector, file_handler=self.file_handler)
            logger.info(f"DB í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ (PDF ë³€í™˜ ì§€ì›)")
        
        except ImportError as e:
            logger.error(f"DB ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            logger.error("í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install sqlalchemy psycopg2-binary pymysql")
            self.data_source = 'excel'
        except Exception as e:
            logger.error(f"DB í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.data_source = 'excel'
    
    def is_revision_newer(self, old_rev: str, new_rev: str) -> bool:
        """
        ë‘ revisionì„ ë¹„êµí•˜ì—¬ ìƒˆ ë²„ì „ì¸ì§€ íŒë‹¨
        
        Args:
            old_rev: ê¸°ì¡´ revision
            new_rev: ìƒˆ revision
        
        Returns:
            True if new_revê°€ old_revë³´ë‹¤ ìµœì‹ 
            
        Note:
            - REV í˜•ì‹: A, A1, C1, D4 (ì•ŒíŒŒë²³ + ìˆ«ì)
            - ì‘ì„±ë²„ì „ í˜•ì‹: R1, R0, R16 (R + ìˆ«ì)
            - ì  ë²„ì „: 1.0, 2.0, 1.1.0
        """
        if old_rev == new_rev:
            return False
        
        import re
        
        try:
            # 1. ì‘ì„±ë²„ì „ í˜•ì‹: R + ìˆ«ì (ì˜ˆ: R1, R0, R16)
            if old_rev.upper().startswith('R') and new_rev.upper().startswith('R'):
                try:
                    old_num = int(old_rev[1:])
                    new_num = int(new_rev[1:])
                    result = new_num > old_num
                    logger.debug(f"ì‘ì„±ë²„ì „ ë¹„êµ: {old_rev}({old_num}) vs {new_rev}({new_num}) â†’ {'ìµœì‹ ' if result else 'ë™ì¼/ì´ì „'}")
                    return result
                except ValueError:
                    pass
            
            # 2. REV í˜•ì‹: ì•ŒíŒŒë²³ + ìˆ«ì (ì˜ˆ: A, A1, C1, D4)
            # íŒ¨í„´: ì•ŒíŒŒë²³(ëŒ€ë¬¸ì) + ì„ íƒì  ìˆ«ì
            rev_pattern = re.compile(r'^([A-Z]+)(\d*)$', re.IGNORECASE)
            old_match = rev_pattern.match(old_rev)
            new_match = rev_pattern.match(new_rev)
            
            if old_match and new_match:
                old_letter = old_match.group(1).upper()
                old_number = int(old_match.group(2)) if old_match.group(2) else 0
                new_letter = new_match.group(1).upper()
                new_number = int(new_match.group(2)) if new_match.group(2) else 0
                
                # ì•ŒíŒŒë²³ ë¨¼ì € ë¹„êµ
                if new_letter > old_letter:
                    logger.debug(f"REV ë¹„êµ: {old_rev}({old_letter}{old_number}) vs {new_rev}({new_letter}{new_number}) â†’ ìµœì‹  (ì•ŒíŒŒë²³)")
                    return True
                elif new_letter < old_letter:
                    logger.debug(f"REV ë¹„êµ: {old_rev}({old_letter}{old_number}) vs {new_rev}({new_letter}{new_number}) â†’ ì´ì „ (ì•ŒíŒŒë²³)")
                    return False
                else:
                    # ì•ŒíŒŒë²³ì´ ê°™ìœ¼ë©´ ìˆ«ì ë¹„êµ
                    result = new_number > old_number
                    logger.debug(f"REV ë¹„êµ: {old_rev}({old_letter}{old_number}) vs {new_rev}({new_letter}{new_number}) â†’ {'ìµœì‹ ' if result else 'ë™ì¼/ì´ì „'} (ìˆ«ì)")
                    return result
            
            # 3. ì  ë²„ì „ í˜•ì‹ ë¹„êµ (1.0, 2.0, 1.1.0)
            if '.' in old_rev or '.' in new_rev:
                old_parts = old_rev.split('.')
                new_parts = new_rev.split('.')
                
                # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ê²½ìš°
                try:
                    for i in range(max(len(old_parts), len(new_parts))):
                        old_num = int(old_parts[i]) if i < len(old_parts) else 0
                        new_num = int(new_parts[i]) if i < len(new_parts) else 0
                        
                        if new_num > old_num:
                            return True
                        elif new_num < old_num:
                            return False
                    
                    # ëª¨ë‘ ê°™ìœ¼ë©´ False
                    return False
                
                except (ValueError, IndexError):
                    # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨
                    pass
            
            # 4. ìˆœìˆ˜ ìˆ«ì ë¹„êµ
            try:
                return float(new_rev) > float(old_rev)
            except ValueError:
                pass
            
            # 5. ë¬¸ìì—´ ì‚¬ì „ì‹ ë¹„êµ (í´ë°±)
            logger.debug(f"Revision ë¹„êµ (ì‚¬ì „ì‹): {old_rev} vs {new_rev}")
            return new_rev > old_rev
        
        except Exception as e:
            logger.warning(f"Revision ë¹„êµ ì‹¤íŒ¨ (old: {old_rev}, new: {new_rev}): {e}")
            # ë¹„êµ ì‹¤íŒ¨ ì‹œ ì—…ë°ì´íŠ¸ë¡œ ê°„ì£¼
            return True
    
    def process(self):
        """ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("="*80)
        logger.info("ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        logger.info(f"ë°ì´í„° ì†ŒìŠ¤: {self.data_source.upper()}")
        if self.data_source in ['excel', 'both']:
            logger.info(f"ì—‘ì…€ íŒŒì¼: {self.excel_path}")
        logger.info(f"Revision ê´€ë¦¬: {'í™œì„±í™”' if ENABLE_REVISION_MANAGEMENT else 'ë¹„í™œì„±í™”'}")
        logger.info("="*80)
        
        try:
            # ë°ì´í„° ìˆ˜ì§‘
            all_data = {}
            
            # 1. Excel ë°ì´í„° ì¶”ì¶œ
            if self.data_source in ['excel', 'both'] and self.excel_processor:
                logger.info("\n[Excel ë°ì´í„° ì²˜ë¦¬]")
                sheet_data = self.excel_processor.process_all_sheets()
                all_data.update(sheet_data)
                self.stats['total_sheets'] += len(sheet_data)
            
            # 2. DB ë°ì´í„° ì¶”ì¶œ
            if self.data_source in ['db', 'both'] and self.db_processor:
                logger.info("\n[DB ë°ì´í„° ì²˜ë¦¬]")
                db_data = self.db_processor.process(query_name="DB_Query")
                # DB ë°ì´í„°ëŠ” ê¸°ì¡´ í˜•ì‹ì´ë¯€ë¡œ ë³€í™˜
                for sheet_name, items in db_data.items():
                    all_data[sheet_name] = (SheetType.ATTACHMENT, items, [])
                self.stats['total_sheets'] += len(db_data)
            
            if not all_data:
                logger.error("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 3. ì‹œíŠ¸ íƒ€ì…ë³„ë¡œ ì²˜ë¦¬
            for sheet_name, (sheet_type, items, headers) in all_data.items():
                logger.info(f"\n{'='*60}")
                logger.info(f"ì‹œíŠ¸ ì²˜ë¦¬ ì‹œì‘: {sheet_name} (íƒ€ì…: {sheet_type.value})")
                logger.info(f"{'='*60}")
                
                # ì‹œíŠ¸ íƒ€ì…ë³„ ë¶„ê¸° ì²˜ë¦¬
                if sheet_type == SheetType.TOC:
                    # ëª©ì°¨ ì‹œíŠ¸ - ê±´ë„ˆë›°ê¸°
                    logger.info(f"[{sheet_name}] ëª©ì°¨ ì‹œíŠ¸ì…ë‹ˆë‹¤. ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                    self.stats['skipped_sheets'] += 1
                
                elif sheet_type in [SheetType.REV_MANAGED, SheetType.VERSION_MANAGED]:
                    # REV/ì‘ì„±ë²„ì „ ê´€ë¦¬ ì‹œíŠ¸
                    self.stats['revision_sheets'] += 1
                    self.process_sheet_with_revision(sheet_name, sheet_type, items, headers)
                
                elif sheet_type == SheetType.ATTACHMENT:
                    # ì²¨ë¶€íŒŒì¼ ì‹œíŠ¸ (ê¸°ì¡´ ë°©ì‹)
                    self.stats['attachment_sheets'] += 1
                    self.process_sheet_attachments(sheet_name, items)
                
                elif sheet_type in [SheetType.HISTORY, SheetType.SOFTWARE]:
                    # ì´ë ¥ê´€ë¦¬/ì†Œí”„íŠ¸ì›¨ì–´ í˜•ìƒê¸°ë¡ ì‹œíŠ¸
                    self.stats['history_sheets'] += 1
                    self.process_sheet_as_text(sheet_name, sheet_type)
                
                elif sheet_type == SheetType.UNKNOWN:
                    # ë¯¸ë¶„ë¥˜ ì‹œíŠ¸ - ì²¨ë¶€íŒŒì¼ë¡œ ì²˜ë¦¬
                    logger.warning(f"[{sheet_name}] ë¯¸ë¶„ë¥˜ ì‹œíŠ¸ì…ë‹ˆë‹¤. ì²¨ë¶€íŒŒì¼ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                    self.stats['attachment_sheets'] += 1
                    self.process_sheet_attachments(sheet_name, items)
            
            # 4. ì„ì‹œ íŒŒì¼ ì •ë¦¬
            self.file_handler.cleanup_temp()
            
            # 5. í†µê³„ ì¶œë ¥
            self.print_statistics()
        
        except Exception as e:
            logger.error(f"ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if self.excel_processor:
                self.excel_processor.close()
            if self.db_processor and self.db_processor.connector:
                self.db_processor.connector.close()
            
            logger.info("="*80)
            logger.info("ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")
            logger.info("="*80)
    
    def process_sheet_with_revision(
        self, 
        sheet_name: str, 
        sheet_type: SheetType,
        items: List[Dict], 
        headers: List[str],
        monitor_progress: bool = True
    ):
        """
        Revision ê´€ë¦¬ ì‹œíŠ¸ ì²˜ë¦¬ (REV/ì‘ì„±ë²„ì „)
        
        Args:
            sheet_name: ì‹œíŠ¸ ì´ë¦„
            sheet_type: ì‹œíŠ¸ íƒ€ì…
            items: í•­ëª© ëª©ë¡ (document_key, revision í¬í•¨)
            headers: í—¤ë” ë¦¬ìŠ¤íŠ¸
            monitor_progress: íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì—¬ë¶€
        """
        if not items:
            logger.warning(f"ì‹œíŠ¸ '{sheet_name}'ì— ì²˜ë¦¬í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"[{sheet_name}] Revision ê´€ë¦¬ ì‹œíŠ¸ ì²˜ë¦¬ ì‹œì‘ (í•­ëª© ìˆ˜: {len(items)})")
        
        try:
            # ì§€ì‹ë² ì´ìŠ¤ ìƒì„±
            dataset_name = f"{sheet_name}"
            dataset_description = f"ì—‘ì…€ ì‹œíŠ¸ '{sheet_name}'ì—ì„œ ìë™ ìƒì„±ëœ ì§€ì‹ë² ì´ìŠ¤ (Revision ê´€ë¦¬)"
            
            dataset = self.ragflow_client.get_or_create_dataset(
                name=dataset_name,
                description=dataset_description,
                permission=DATASET_PERMISSION,
                embedding_model=None,
                chunk_method=CHUNK_METHOD,
                parser_config=PARSER_CONFIG
            )
            
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {sheet_name}")
                return
            
            self.stats['datasets_created'] += 1
            
            # Revision ê´€ë¦¬ê°€ í™œì„±í™”ëœ ê²½ìš°: RevisionDBì—ì„œ ê¸°ì¡´ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
            existing_docs_map = {}  # document_key -> {doc_id, revision, ...}
            dataset_id = dataset.get('id')
            
            if ENABLE_REVISION_MANAGEMENT:
                logger.info(f"[{sheet_name}] RevisionDBì—ì„œ ê¸°ì¡´ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘...")
                db_docs = self.revision_db.get_all_documents(dataset_id=dataset_id)
                
                # ë¬¸ì„œë¥¼ document_keyë¡œ ë§¤í•‘
                for doc in db_docs:
                    doc_key = doc.get('document_key')
                    if doc_key:
                        existing_docs_map[doc_key] = {
                            'doc_id': doc.get('document_id'),
                            'revision': doc.get('revision'),
                            'name': doc.get('file_name')
                        }
                
                logger.info(f"[{sheet_name}] RevisionDBì—ì„œ ê¸°ì¡´ ë¬¸ì„œ {len(existing_docs_map)}ê°œ ë°œê²¬")
            
            # ê° í•­ëª© ì²˜ë¦¬
            uploaded_count = 0
            for item in items:
                document_key = item.get('document_key')
                new_revision = item.get('revision')
                
                if not document_key:
                    logger.warning(f"í–‰ {item.get('row_number')}: document_keyê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # Revision ë¹„êµ ë° ì²˜ë¦¬
                if ENABLE_REVISION_MANAGEMENT and document_key in existing_docs_map:
                    existing_info = existing_docs_map[document_key]
                    old_revision = existing_info.get('revision')
                    
                    # Revision ë¹„êµ
                    if old_revision and new_revision:
                        if old_revision == new_revision:
                            # ë™ì¼ ë²„ì „ - ê±´ë„ˆë›°ê¸°
                            if SKIP_SAME_REVISION:
                                logger.info(f"  [{document_key}] ë™ì¼ revision ({new_revision}) - ê±´ë„ˆëœ€")
                                self.stats['skipped_documents'] += 1
                                continue
                        elif not self.is_revision_newer(old_revision, new_revision):
                            # ì´ì „ ë²„ì „ - ê±´ë„ˆë›°ê¸°
                            logger.info(f"  [{document_key}] ì´ì „ revision ({new_revision} <= {old_revision}) - ê±´ë„ˆëœ€")
                            self.stats['skipped_documents'] += 1
                            continue
                        else:
                            # ì—…ë°ì´íŠ¸ í•„ìš”
                            logger.info(f"  [{document_key}] Revision ì—…ë°ì´íŠ¸: {old_revision} â†’ {new_revision}")
                            
                            # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
                            if DELETE_BEFORE_UPLOAD:
                                doc_id = existing_info.get('doc_id')
                                if self.ragflow_client.delete_document(dataset, doc_id):
                                    self.stats['deleted_documents'] += 1
                                    logger.info(f"    âœ“ RAGFlowì—ì„œ ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
                                    # RevisionDBì—ì„œë„ ì‚­ì œ
                                    self.revision_db.delete_document(document_key, dataset_id)
                                    logger.debug(f"    âœ“ RevisionDBì—ì„œë„ ì‚­ì œ ì™„ë£Œ")
                                else:
                                    self.stats['failed_deletions'] += 1
                                    logger.error(f"    âœ— ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                                    continue
                    else:
                        logger.debug(f"  [{document_key}] Revision ì •ë³´ ë¶ˆì™„ì „ - ì—…ë°ì´íŠ¸ ì§„í–‰")
                    
                    # íŒŒì¼ ì—…ë¡œë“œ
                    if self.process_item(dataset, item):
                        uploaded_count += 1
                        self.stats['updated_documents'] += 1
                        logger.info(f"    âœ“ ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                
                else:
                    # ì‹ ê·œ ë¬¸ì„œ
                    logger.info(f"  [{document_key}] ì‹ ê·œ ë¬¸ì„œ (revision: {new_revision})")
                    if self.process_item(dataset, item):
                        uploaded_count += 1
                        self.stats['new_documents'] += 1
                        logger.info(f"    âœ“ ì‹ ê·œ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ")
            
            # ì¼ê´„ íŒŒì‹± ì‹œì‘
            if uploaded_count > 0:
                logger.info(f"[{sheet_name}] {uploaded_count}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ, ì¼ê´„ íŒŒì‹± ì‹œì‘")
                parse_started = self.ragflow_client.start_batch_parse(dataset)
                
                if parse_started and monitor_progress and MONITOR_PARSE_PROGRESS:
                    self.monitor_parse_progress(dataset, sheet_name, max_wait_minutes=PARSE_TIMEOUT_MINUTES)
                elif parse_started:
                    logger.info(f"[{sheet_name}] íŒŒì‹±ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
            else:
                logger.info(f"[{sheet_name}] ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.info(f"[{sheet_name}] Revision ê´€ë¦¬ ì‹œíŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
        
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def process_sheet_attachments(self, sheet_name: str, items: List[Dict], monitor_progress: bool = False):
        """
        ì²¨ë¶€íŒŒì¼ ì‹œíŠ¸ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹ - Revision ê´€ë¦¬ ì—†ìŒ)
        
        Args:
            sheet_name: ì‹œíŠ¸ ì´ë¦„
            items: í•˜ì´í¼ë§í¬ì™€ ë©”íƒ€ë°ì´í„° ëª©ë¡
            monitor_progress: íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì—¬ë¶€
        """
        if not items:
            logger.warning(f"ì‹œíŠ¸ '{sheet_name}'ì— ì²˜ë¦¬í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"[{sheet_name}] ì²¨ë¶€íŒŒì¼ ì‹œíŠ¸ ì²˜ë¦¬ ì‹œì‘ (í•­ëª© ìˆ˜: {len(items)})")
        
        try:
            # ì§€ì‹ë² ì´ìŠ¤ ìƒì„±
            dataset_name = f"{sheet_name}"
            dataset_description = f"ì—‘ì…€ ì‹œíŠ¸ '{sheet_name}'ì—ì„œ ìë™ ìƒì„±ëœ ì§€ì‹ë² ì´ìŠ¤"
            
            dataset = self.ragflow_client.get_or_create_dataset(
                name=dataset_name,
                description=dataset_description,
                permission=DATASET_PERMISSION,
                embedding_model=None,
                chunk_method=CHUNK_METHOD,
                parser_config=PARSER_CONFIG
            )
            
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {sheet_name}")
                return
            
            self.stats['datasets_created'] += 1
            
            # ê° í•­ëª© ì²˜ë¦¬
            uploaded_count = 0
            for item in items:
                if self.process_item(dataset, item):
                    uploaded_count += 1
            
            # ì¼ê´„ íŒŒì‹± ì‹œì‘
            if uploaded_count > 0:
                logger.info(f"[{sheet_name}] {uploaded_count}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ, ì¼ê´„ íŒŒì‹± ì‹œì‘")
                parse_started = self.ragflow_client.start_batch_parse(dataset)
                
                if parse_started and monitor_progress and MONITOR_PARSE_PROGRESS:
                    self.monitor_parse_progress(dataset, sheet_name, max_wait_minutes=PARSE_TIMEOUT_MINUTES)
                elif parse_started:
                    logger.info(f"[{sheet_name}] íŒŒì‹±ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
            
            logger.info(f"[{sheet_name}] ì²¨ë¶€íŒŒì¼ ì‹œíŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
        
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def process_sheet_as_text(self, sheet_name: str, sheet_type: SheetType, monitor_progress: bool = False):
        """
        ì´ë ¥ê´€ë¦¬/ì†Œí”„íŠ¸ì›¨ì–´ í˜•ìƒê¸°ë¡ ì‹œíŠ¸ë¥¼ í…ìŠ¤íŠ¸ ë˜ëŠ” Excelë¡œ ë³€í™˜í•˜ì—¬ ì—…ë¡œë“œ
        
        Args:
            sheet_name: ì‹œíŠ¸ ì´ë¦„
            sheet_type: ì‹œíŠ¸ íƒ€ì… (HISTORY ë˜ëŠ” SOFTWARE)
            monitor_progress: íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì—¬ë¶€
        """
        upload_format = HISTORY_SHEET_UPLOAD_FORMAT
        logger.info(f"[{sheet_name}] ì‹œíŠ¸ ì²˜ë¦¬ ì‹œì‘ (í˜•ì‹: {upload_format.upper()})")
        
        try:
            # ì§€ì‹ë² ì´ìŠ¤ ìƒì„±
            dataset_name = f"{sheet_name}"
            dataset_description = f"ì—‘ì…€ ì‹œíŠ¸ '{sheet_name}' ({sheet_type.value})"
            
            dataset = self.ragflow_client.get_or_create_dataset(
                name=dataset_name,
                description=dataset_description,
                permission=DATASET_PERMISSION,
                embedding_model=None,
                chunk_method=CHUNK_METHOD,
                parser_config=PARSER_CONFIG
            )
            
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {sheet_name}")
                return
            
            self.stats['datasets_created'] += 1
            uploaded_count = 0
            
            if upload_format == "excel":
                # Excel íŒŒì¼ë¡œ ì¶”ì¶œí•˜ì—¬ ì—…ë¡œë“œ
                logger.info(f"[{sheet_name}] Excel íŒŒì¼ë¡œ ì¶”ì¶œ ì¤‘...")
                excel_file_path = self.excel_processor.extract_sheet_as_excel(sheet_name, TEMP_DIR)
                
                if not excel_file_path:
                    logger.error(f"[{sheet_name}] Excel ì¶”ì¶œ ì‹¤íŒ¨")
                    return
                
                # Excel íŒŒì¼ ì—…ë¡œë“œ
                metadata = {
                    'ì‹œíŠ¸ëª…': sheet_name,
                    'íƒ€ì…': sheet_type.value,
                    'íŒŒì¼í˜•ì‹': 'excel'
                }
                
                success = self.ragflow_client.upload_document(
                    dataset=dataset,
                    file_path=excel_file_path,
                    metadata=metadata,
                    display_name=f"{sheet_name}.xlsx"
                )
                
                if success:
                    uploaded_count += 1
                    self.stats['successful_uploads'] += 1
                    logger.info(f"[{sheet_name}] Excel íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")
                else:
                    self.stats['failed_uploads'] += 1
                    logger.error(f"[{sheet_name}] Excel íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨")
            
            else:  # upload_format == "text" - PDFë¡œ ë³€í™˜í•˜ì—¬ ì—…ë¡œë“œ
                # í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ PDFë¡œ ë³€í™˜ (ì—¬ëŸ¬ ì²­í¬ ê°€ëŠ¥)
                logger.info(f"[{sheet_name}] í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...")
                text_chunks = self.excel_processor.convert_sheet_to_text_chunks(sheet_name)
                
                if not text_chunks:
                    logger.warning(f"[{sheet_name}] ë³€í™˜ëœ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    return
                
                logger.info(f"[{sheet_name}] {len(text_chunks)}ê°œ ì²­í¬ ìƒì„±ë¨")
                
                # ê° ì²­í¬ë¥¼ PDFë¡œ ë³€í™˜í•˜ì—¬ ì—…ë¡œë“œ
                for chunk_idx, chunk_content in enumerate(text_chunks, 1):
                    # íŒŒì¼ëª…: ì²­í¬ê°€ 1ê°œë©´ ë²ˆí˜¸ ì—†ì´, ì—¬ëŸ¬ ê°œë©´ ë²ˆí˜¸ ë¶™ì„
                    if len(text_chunks) == 1:
                        filename = f"{sheet_name}_{sheet_type.value}"
                        display_name = f"{sheet_name}_{sheet_type.value}.pdf"
                    else:
                        filename = f"{sheet_name}_{sheet_type.value}_part{chunk_idx}"
                        display_name = f"{sheet_name}_{sheet_type.value}_part{chunk_idx}.pdf"
                    
                    # í…ìŠ¤íŠ¸ë¥¼ PDFë¡œ ë³€í™˜
                    pdf_file_path = self.file_handler.convert_text_to_pdf(chunk_content, filename)
                    
                    if not pdf_file_path:
                        logger.error(f"[{sheet_name}] ì²­í¬ {chunk_idx} PDF ë³€í™˜ ì‹¤íŒ¨")
                        self.stats['failed_uploads'] += 1
                        continue
                    
                    # PDF íŒŒì¼ ì—…ë¡œë“œ
                    metadata = {
                        'ì‹œíŠ¸ëª…': sheet_name,
                        'íƒ€ì…': sheet_type.value,
                        'íŒŒì¼í˜•ì‹': 'pdf',
                        'ì²­í¬_ë²ˆí˜¸': str(chunk_idx) if len(text_chunks) > 1 else '1',
                        'ì´_ì²­í¬_ìˆ˜': str(len(text_chunks))
                    }
                    
                    success = self.ragflow_client.upload_document(
                        dataset=dataset,
                        file_path=pdf_file_path,
                        metadata=metadata,
                        display_name=display_name
                    )
                    
                    if success:
                        uploaded_count += 1
                        self.stats['successful_uploads'] += 1
                        logger.info(f"[{sheet_name}] ì²­í¬ {chunk_idx}/{len(text_chunks)} PDF ì—…ë¡œë“œ ì™„ë£Œ")
                    else:
                        self.stats['failed_uploads'] += 1
                        logger.error(f"[{sheet_name}] ì²­í¬ {chunk_idx}/{len(text_chunks)} PDF ì—…ë¡œë“œ ì‹¤íŒ¨")
            
            # ì¼ê´„ íŒŒì‹± ì‹œì‘
            if uploaded_count > 0:
                logger.info(f"[{sheet_name}] {uploaded_count}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ, ì¼ê´„ íŒŒì‹± ì‹œì‘")
                parse_started = self.ragflow_client.start_batch_parse(dataset)
                
                if parse_started and monitor_progress and MONITOR_PARSE_PROGRESS:
                    self.monitor_parse_progress(dataset, sheet_name, max_wait_minutes=PARSE_TIMEOUT_MINUTES)
                elif parse_started:
                    logger.info(f"[{sheet_name}] íŒŒì‹±ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
            
            logger.info(f"[{sheet_name}] ì‹œíŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
        
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def process_sheet(self, sheet_name: str, items: List[Dict], monitor_progress: bool = False):
        """
        ì‹œíŠ¸ ë‹¨ìœ„ ì²˜ë¦¬
        
        Args:
            sheet_name: ì‹œíŠ¸ ì´ë¦„
            items: í•˜ì´í¼ë§í¬ì™€ ë©”íƒ€ë°ì´í„° ëª©ë¡
            monitor_progress: íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì—¬ë¶€ (ê¸°ë³¸: False)
        """
        if not items:
            logger.warning(f"ì‹œíŠ¸ '{sheet_name}'ì— ì²˜ë¦¬í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.log_sheet_start(sheet_name)
        
        try:
            # ì‹œíŠ¸ë³„ ì§€ì‹ë² ì´ìŠ¤ ìƒì„±
            dataset_name = f"{sheet_name}"
            dataset_description = f"ì—‘ì…€ ì‹œíŠ¸ '{sheet_name}'ì—ì„œ ìë™ ìƒì„±ëœ ì§€ì‹ë² ì´ìŠ¤"
            
            dataset = self.ragflow_client.get_or_create_dataset(
                name=dataset_name,
                description=dataset_description,
                permission=DATASET_PERMISSION,
                embedding_model=None,  # ì‹œìŠ¤í…œ ê¸°ë³¸ê°’ ì‚¬ìš© (tenant.embd_id)
                chunk_method=CHUNK_METHOD,  # GUIì™€ ë™ì¼í•œ ì²­í¬ ë°©ë²•
                parser_config=PARSER_CONFIG  # GUIì™€ ë™ì¼í•œ íŒŒì„œ ì„¤ì •
            )
            
            if not dataset:
                logger.error(f"ì§€ì‹ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {sheet_name}")
                return
            
            self.stats['datasets_created'] += 1
            
            # ê° í•­ëª© ì²˜ë¦¬
            uploaded_count = 0
            for item in items:
                if self.process_item(dataset, item):
                    uploaded_count += 1
            
            # ì¼ê´„ íŒŒì‹± ì‹œì‘
            if uploaded_count > 0:
                logger.info(f"ì‹œíŠ¸ '{sheet_name}': {uploaded_count}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ, ì¼ê´„ íŒŒì‹± ì‹œì‘")
                parse_started = self.ragflow_client.start_batch_parse(dataset)
                
                # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ (ì˜µì…˜)
                if parse_started and monitor_progress:
                    self.monitor_parse_progress(dataset, sheet_name, max_wait_minutes=PARSE_TIMEOUT_MINUTES)
                elif parse_started:
                    logger.info(f"ì‹œíŠ¸ '{sheet_name}': íŒŒì‹±ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤. Management UIì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
            
            logger.log_sheet_end(sheet_name, uploaded_count)
        
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def process_item(self, dataset: object, item: Dict) -> Optional[str]:
        """
        ê°œë³„ í•­ëª© ì²˜ë¦¬ (íŒŒì¼ ë‹¤ìš´ë¡œë“œ, ë³€í™˜, ì—…ë¡œë“œ)
        
        Args:
            dataset: Dataset ê°ì²´
            item: {'hyperlink': '...', 'metadata': {...}, 'document_key': '...', 'revision': '...', ...}
        
        Returns:
            ë¬¸ì„œ ID (ì„±ê³µ ì‹œ) ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        hyperlink = item.get('hyperlink')
        metadata = item.get('metadata', {})
        row_number = item.get('row_number')
        document_key = item.get('document_key')
        revision = item.get('revision')
        
        if not hyperlink:
            logger.warning(f"{row_number}í–‰: í•˜ì´í¼ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        self.stats['total_files'] += 1
        
        try:
            # 1. íŒŒì¼ ê°€ì ¸ì˜¤ê¸° (ë‹¤ìš´ë¡œë“œ ë˜ëŠ” ë³µì‚¬)
            file_path = self.file_handler.get_file(hyperlink)
            
            if not file_path:
                logger.error(f"{row_number}í–‰: íŒŒì¼ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ - {hyperlink}")
                self.stats['failed_uploads'] += 1
                return None
            
            # 2. íŒŒì¼ ì²˜ë¦¬ (í˜•ì‹ ë³€í™˜)
            processed_files = self.file_handler.process_file(file_path)
            
            if not processed_files:
                logger.error(f"{row_number}í–‰: íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ - {file_path.name}")
                self.stats['failed_uploads'] += 1
                return None
            
            # 3. ì²˜ë¦¬ëœ íŒŒì¼ë“¤ì„ RAGFlowì— ì—…ë¡œë“œ
            document_id = None
            for processed_path, file_type in processed_files:
                # ë©”íƒ€ë°ì´í„°ì— ì›ë³¸ ì •ë³´ ì¶”ê°€
                enhanced_metadata = metadata.copy()
                enhanced_metadata['ì›ë³¸_íŒŒì¼'] = file_path.name
                enhanced_metadata['íŒŒì¼_í˜•ì‹'] = file_type
                enhanced_metadata['ì—‘ì…€_í–‰ë²ˆí˜¸'] = str(row_number)
                enhanced_metadata['í•˜ì´í¼ë§í¬'] = hyperlink
                
                # Revision ê´€ë¦¬ ì •ë³´ ì¶”ê°€
                if document_key:
                    enhanced_metadata['document_key'] = document_key
                if revision:
                    enhanced_metadata['revision'] = revision
                
                # ì—…ë¡œë“œ (document_id ë°˜í™˜)
                doc_id = self.ragflow_client.upload_document(
                    dataset=dataset,
                    file_path=processed_path,
                    metadata=enhanced_metadata,
                    display_name=processed_path.name
                )
                
                if doc_id:
                    document_id = doc_id  # ì²« ë²ˆì§¸ ì„±ê³µí•œ ë¬¸ì„œ ID ì‚¬ìš©
                    self.stats['successful_uploads'] += 1
                    logger.log_file_process(
                        processed_path.name, 
                        "ì—…ë¡œë“œ ì„±ê³µ",
                        f"í˜•ì‹: {file_type}, í–‰: {row_number}, ë¬¸ì„œID: {doc_id}"
                    )
                    
                    # RevisionDBì— ì €ì¥ (revision ê´€ë¦¬ê°€ í™œì„±í™”ëœ ê²½ìš°)
                    if ENABLE_REVISION_MANAGEMENT and document_key:
                        dataset_id = dataset.get('id')
                        dataset_name = dataset.get('name')
                        self.revision_db.save_document(
                            document_key=document_key,
                            document_id=doc_id,
                            dataset_id=dataset_id,
                            dataset_name=dataset_name,
                            revision=revision,
                            file_path=str(processed_path),
                            file_name=processed_path.name
                        )
                        logger.debug(f"RevisionDBì— ì €ì¥: {document_key} â†’ {doc_id}")
                else:
                    self.stats['failed_uploads'] += 1
                    logger.log_file_process(
                        processed_path.name, 
                        "ì—…ë¡œë“œ ì‹¤íŒ¨",
                        f"í˜•ì‹: {file_type}, í–‰: {row_number}"
                    )
            
            return document_id
        
        except Exception as e:
            logger.error(f"{row_number}í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.stats['failed_uploads'] += 1
            return None
    
    def monitor_parse_progress(self, dataset: Dict, dataset_name: str, max_wait_minutes: int = 30):
        """
        íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ (Management API ì „ìš©)
        
        Args:
            dataset: Dataset ë”•ì…”ë„ˆë¦¬
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ (ë¡œê·¸ìš©)
            max_wait_minutes: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ë¶„, ê¸°ë³¸: 30ë¶„)
        """
        logger.info(f"[{dataset_name}] ğŸ“Š íŒŒì‹± ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
        logger.info(f"[{dataset_name}] ìµœëŒ€ ëŒ€ê¸° ì‹œê°„: {max_wait_minutes}ë¶„")
        
        start_time = time.time()
        max_wait_seconds = max_wait_minutes * 60
        check_interval = 10  # 10ì´ˆë§ˆë‹¤ í™•ì¸
        last_status = None
        
        while True:
            try:
                # ì§„í–‰ ìƒí™© ì¡°íšŒ
                progress = self.ragflow_client.get_parse_progress(dataset)
                
                if progress:
                    status = progress.get('status', 'unknown')
                    current = progress.get('current_document_index', 0)
                    total = progress.get('total_documents', 0)
                    current_doc = progress.get('current_document_name', 'N/A')
                    
                    # ìƒíƒœ ë³€ê²½ ì‹œì—ë§Œ ë¡œê·¸ ì¶œë ¥ (ì¤‘ë³µ ë°©ì§€)
                    if status != last_status or current != getattr(self, '_last_current', -1):
                        if total > 0:
                            progress_percent = (current / total) * 100
                            logger.info(
                                f"[{dataset_name}] ğŸ“„ ì§„í–‰: {current}/{total} ({progress_percent:.1f}%) "
                                f"| ìƒíƒœ: {status} | í˜„ì¬: {current_doc}"
                            )
                        else:
                            logger.info(f"[{dataset_name}] ìƒíƒœ: {status}")
                        
                        last_status = status
                        self._last_current = current
                    
                    # ì™„ë£Œ ì²´í¬
                    if status == 'completed' or (total > 0 and current >= total):
                        logger.info(f"[{dataset_name}] âœ“ íŒŒì‹± ì™„ë£Œ!")
                        logger.info(f"[{dataset_name}] ì´ {total}ê°œ ë¬¸ì„œ íŒŒì‹± ì™„ë£Œ")
                        break
                    
                    elif status == 'error':
                        error_msg = progress.get('error_message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                        logger.error(f"[{dataset_name}] âœ— íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
                        break
                    
                    elif status == 'idle' and current == 0:
                        logger.warning(f"[{dataset_name}] âš ï¸ íŒŒì‹±ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                else:
                    logger.debug(f"[{dataset_name}] ì§„í–‰ ìƒí™© ì •ë³´ ì—†ìŒ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ëŒ€ê¸° ì¤‘...)")
                
                # íƒ€ì„ì•„ì›ƒ ì²´í¬
                elapsed = time.time() - start_time
                if elapsed > max_wait_seconds:
                    logger.warning(f"[{dataset_name}] â±ï¸ íŒŒì‹± ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ ({max_wait_minutes}ë¶„)")
                    logger.info(f"[{dataset_name}] íŒŒì‹±ì€ ê³„ì† ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. Management UIì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
                    break
                
                # ëŒ€ê¸°
                time.sleep(check_interval)
            
            except Exception as e:
                logger.error(f"[{dataset_name}] ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                logger.info(f"[{dataset_name}] Management UIì—ì„œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.")
                break
        
        # ìµœì¢… ìƒíƒœ í™•ì¸
        try:
            final_progress = self.ragflow_client.get_parse_progress(dataset)
            if final_progress:
                final_status = final_progress.get('status', 'unknown')
                logger.info(f"[{dataset_name}] ìµœì¢… ìƒíƒœ: {final_status}")
        except:
            pass
    
    def print_statistics(self):
        """ì²˜ë¦¬ í†µê³„ ì¶œë ¥"""
        logger.info("="*80)
        logger.info("ë°°ì¹˜ ì²˜ë¦¬ í†µê³„")
        logger.info("-"*80)
        
        # ì‹œíŠ¸ í†µê³„
        logger.info(f"ì´ ì‹œíŠ¸ ìˆ˜: {self.stats['total_sheets']}")
        logger.info(f"  - ê±´ë„ˆë›´ ì‹œíŠ¸ (ëª©ì°¨): {self.stats['skipped_sheets']}")
        logger.info(f"  - Revision ê´€ë¦¬ ì‹œíŠ¸: {self.stats['revision_sheets']}")
        logger.info(f"  - ì²¨ë¶€íŒŒì¼ ì‹œíŠ¸: {self.stats['attachment_sheets']}")
        logger.info(f"  - ì´ë ¥ê´€ë¦¬/ì†Œí”„íŠ¸ì›¨ì–´ ì‹œíŠ¸: {self.stats['history_sheets']}")
        logger.info(f"ìƒì„±ëœ ì§€ì‹ë² ì´ìŠ¤ ìˆ˜: {self.stats['datasets_created']}")
        
        logger.info("-"*80)
        
        # Revision ê´€ë¦¬ í†µê³„
        if self.stats['revision_sheets'] > 0:
            logger.info(f"Revision ê´€ë¦¬ ë¬¸ì„œ:")
            logger.info(f"  - ì‹ ê·œ ë¬¸ì„œ: {self.stats['new_documents']}")
            logger.info(f"  - ì—…ë°ì´íŠ¸ ë¬¸ì„œ: {self.stats['updated_documents']}")
            logger.info(f"  - ê±´ë„ˆë›´ ë¬¸ì„œ (ë™ì¼ revision): {self.stats['skipped_documents']}")
            logger.info(f"  - ì‚­ì œëœ ë¬¸ì„œ: {self.stats['deleted_documents']}")
            if self.stats['failed_deletions'] > 0:
                logger.info(f"  - ì‚­ì œ ì‹¤íŒ¨: {self.stats['failed_deletions']}")
            logger.info("-"*80)
        
        # íŒŒì¼ ì—…ë¡œë“œ í†µê³„
        logger.info(f"ì´ íŒŒì¼ ìˆ˜: {self.stats['total_files']}")
        logger.info(f"ì—…ë¡œë“œ ì„±ê³µ: {self.stats['successful_uploads']}")
        logger.info(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {self.stats['failed_uploads']}")
        
        if self.stats['total_files'] > 0:
            success_rate = (self.stats['successful_uploads'] / self.stats['total_files']) * 100
            logger.info(f"ì—…ë¡œë“œ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        logger.info("="*80)

